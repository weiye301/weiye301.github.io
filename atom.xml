<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一叶扁粥~</title>
  
  <subtitle>学无止境，心存敬畏！！！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://weiye301.github.io/"/>
  <updated>2020-02-12T01:21:06.628Z</updated>
  <id>https://weiye301.github.io/</id>
  
  <author>
    <name>一叶扁粥</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Spark数据倾斜及解决方案</title>
    <link href="https://weiye301.github.io/2020/02/12/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/"/>
    <id>https://weiye301.github.io/2020/02/12/Spark%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/</id>
    <published>2020-02-12T01:19:33.000Z</published>
    <updated>2020-02-12T01:21:06.628Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、什么是数据倾斜"><a href="#一、什么是数据倾斜" class="headerlink" title="一、什么是数据倾斜"></a>一、什么是数据倾斜</h1><p>对 Spark/Hadoop 这样的分布式大数据系统来讲，数据量大并不可怕，可怕的是数据倾斜。</p><p>对于分布式系统而言，理想情况下，随着系统规模（节点数量）的增加，应用整体耗时线性下降。如果一台机器处理一批大量数据需要120分钟，当机器数量增加到3台时，理想的耗时为120 / 3 = 40分钟。但是，想做到分布式情况下每台机器执行时间是单机时的1 / N，就必须保证每台机器的任务量相等。不幸的是，很多时候，任务的分配是不均匀的，甚至不均匀到大部分任务被分配到个别机器上，其它大部分机器所分配的任务量只占总得的小部分。比如一台机器负责处理 80% 的任务，另外两台机器各处理 10% 的任务。</p><p>『不患多而患不均』，这是分布式环境下最大的问题。意味着计算能力不是线性扩展的，而是存在短板效应: 一个 Stage 所耗费的时间，是由最慢的那个 Task 决定。</p><p>由于同一个 Stage 内的所有 task 执行相同的计算，在排除不同计算节点计算能力差异的前提下，不同 task 之间耗时的差异主要由该 task 所处理的数据量决定。所以，要想发挥分布式系统并行计算的优势，就必须解决数据倾斜问题。</p><a id="more"></a><h1 id="二、数据倾斜的危害"><a href="#二、数据倾斜的危害" class="headerlink" title="二、数据倾斜的危害"></a>二、数据倾斜的危害</h1><p>当出现数据倾斜时，小量任务耗时远高于其它任务，从而使得整体耗时过大，未能充分发挥分布式系统的并行计算优势。　　</p><p>另外，当发生数据倾斜时，部分任务处理的数据量过大，可能造成内存不足使得任务失败，并进而引进整个应用失败。　　</p><h1 id="三、数据倾斜的现象"><a href="#三、数据倾斜的现象" class="headerlink" title="三、数据倾斜的现象"></a>三、数据倾斜的现象</h1><p>当发现如下现象时，十有八九是发生数据倾斜了:</p><ul><li>绝大多数 task 执行得都非常快，但个别 task 执行极慢，整体任务卡在某个阶段不能结束。</li><li>原本能够正常执行的 Spark 作业，某天突然报出 OOM（内存溢出）异常，观察异常栈，是我们写的业务代码造成的。这种情况比较少见。</li></ul><pre><code>在 Spark streaming 程序中，数据倾斜更容易出现，特别是在程序中包含一些类似 sql 的 join、group 这种操作的时候。因为 Spark Streaming 程序在运行的时候，我们一般不会分配特别多的内存，因此一旦在这个过程中出现一些数据倾斜，就十分容易造成 OOM。</code></pre><h1 id="四、数据倾斜的原因"><a href="#四、数据倾斜的原因" class="headerlink" title="四、数据倾斜的原因"></a>四、数据倾斜的原因</h1><p>在进行 shuffle 的时候，必须将各个节点上相同的 key 拉取到某个节点上的一个 task 来进行处理，比如按照 key 进行聚合或 join 等操作。此时如果某个 key 对应的数据量特别大的话，就会发生数据倾斜。比如大部分 key 对应10条数据，但是个别 key 却对应了100万条数据，那么大部分 task 可能就只会分配到10条数据，然后1秒钟就运行完了；但是个别 task 可能分配到了100万数据，要运行一两个小时。</p><p>因此出现数据倾斜的时候，Spark 作业看起来会运行得非常缓慢，甚至可能因为某个 task 处理的数据量过大导致内存溢出。</p><h1 id="五、问题发现与定位"><a href="#五、问题发现与定位" class="headerlink" title="五、问题发现与定位"></a>五、问题发现与定位</h1><p><strong>1、通过 Spark Web UI</strong></p><p>通过 Spark Web UI 来查看当前运行的 stage 各个 task 分配的数据量（Shuffle Read Size/Records），从而进一步确定是不是 task 分配的数据不均匀导致了数据倾斜。</p><p>知道数据倾斜发生在哪一个 stage 之后，接着我们就需要根据 stage 划分原理，推算出来发生倾斜的那个 stage 对应代码中的哪一部分，这部分代码中肯定会有一个 shuffle 类算子。可以通过 countByKey 查看各个 key 的分布。</p><pre><code>数据倾斜只会发生在 shuffle 过程中。这里给大家罗列一些常用的并且可能会触发 shuffle 操作的算子: distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition 等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</code></pre><p><strong>2、通过 key 统计</strong></p><p>也可以通过抽样统计 key 的出现次数验证。</p><p>由于数据量巨大，可以采用抽样的方式，对数据进行抽样，统计出现的次数，根据出现次数大小排序取出前几个:</p><pre><code>df.select(&quot;key&quot;).sample(false, 0.1)           // 数据采样        .(k =&gt; (k, 1)).reduceBykey(_ + _)         // 统计 key 出现的次数         .map(k =&gt; (k._2, k._1)).sortByKey(false)  // 根据 key 出现次数进行排序        .take(10)                                 // 取前 10 个。</code></pre><p>如果发现多数数据分布都较为平均，而个别数据比其他数据大上若干个数量级，则说明发生了数据倾斜。</p><h1 id="六、如何缓解数据倾斜"><a href="#六、如何缓解数据倾斜" class="headerlink" title="六、如何缓解数据倾斜"></a>六、如何缓解数据倾斜</h1><p><strong>基本思路</strong></p><ul><li>业务逻辑: 我们从业务逻辑的层面上来优化数据倾斜，比如要统计不同城市的订单情况，那么我们单独对这一线城市来做 count，最后和其它城市做整合。</li><li>程序实现: 比如说在 Hive 中，经常遇到 count（distinct）操作，这样会导致最终只有一个 reduce，我们可以先 group 再在外面包一层 count，就可以了；在 Spark 中使用 reduceByKey 替代 groupByKey 等。</li><li>参数调优: Hadoop 和 Spark 都自带了很多的参数和机制来调节数据倾斜，合理利用它们就能解决大部分问题。</li></ul><p><strong>思路1. 过滤异常数据</strong></p><p>如果导致数据倾斜的 key 是异常数据，那么简单的过滤掉就可以了。</p><p>首先要对 key 进行分析，判断是哪些 key 造成数据倾斜。具体方法上面已经介绍过了，这里不赘述。</p><p>然后对这些 key 对应的记录进行分析:</p><ol><li>空值或者异常值之类的，大多是这个原因引起</li><li>无效数据，大量重复的测试数据或是对结果影响不大的有效数据</li><li>有效数据，业务导致的正常数据分布</li></ol><p>解决方案</p><p>对于第 1，2 种情况，直接对数据进行过滤即可。</p><p>第3种情况则需要特殊的处理，具体我们下面详细介绍。</p><p><strong>思路2. 提高 shuffle 并行度</strong></p><p>Spark 在做 Shuffle 时，默认使用 HashPartitioner（非 Hash Shuffle）对数据进行分区。如果并行度设置的不合适，可能造成大量不相同的 Key 对应的数据被分配到了同一个 Task 上，造成该 Task 所处理的数据远大于其它 Task，从而造成数据倾斜。</p><p>如果调整 Shuffle 时的并行度，使得原本被分配到同一 Task 的不同 Key 发配到不同 Task 上处理，则可降低原 Task 所需处理的数据量，从而缓解数据倾斜问题造成的短板效应。</p><p>（1）操作流程</p><p>RDD 操作 可在需要 Shuffle 的操作算子上直接设置并行度或者使用 spark.default.parallelism 设置。如果是 Spark SQL，还可通过</p><pre><code> SET spark.sql.shuffle.partitions=[num_tasks] </code></pre><p>设置并行度。默认参数由不同的 Cluster Manager 控制。</p><p>dataFrame 和 sparkSql 可以设置 </p><pre><code>spark.sql.shuffle.partitions=[num_tasks]</code></pre><p> 参数控制 shuffle 的并发度，默认为200。</p><p>（2）适用场景</p><p>大量不同的 Key 被分配到了相同的 Task 造成该 Task 数据量过大。</p><p>（3）解决方案</p><p>调整并行度。一般是增大并行度，但有时如减小并行度也可达到效果。</p><p>（4）优势</p><p>实现简单，只需要参数调优。可用最小的代价解决问题。一般如果出现数据倾斜，都可以通过这种方法先试验几次，如果问题未解决，再尝试其它方法。</p><p>（5）劣势</p><p>适用场景少，只是让每个 task 执行更少的不同的key。无法解决个别key特别大的情况造成的倾斜，如果某些 key 的大小非常大，即使一个 task 单独执行它，也会受到数据倾斜的困扰。并且该方法一般只能缓解数据倾斜，没有彻底消除问题。从实践经验来看，其效果一般。</p><pre><code>TIPS 可以把数据倾斜类比为 hash 冲突。提高并行度就类似于 提高 hash 表的大小。</code></pre><p><strong>思路3. 自定义 Partitioner</strong></p><p>（1）原理</p><p>使用自定义的 Partitioner（默认为 HashPartitioner），将原本被分配到同一个 Task 的不同 Key 分配到不同 Task。</p><p>例如，我们在 groupByKey 算子上，使用自定义的 Partitioner:</p><pre><code class="java">.groupByKey(new Partitioner() {  @Override  public int numPartitions() {    return 12;  }  @Override  public int getPartition(Object key) {    int id = Integer.parseInt(key.toString());    if(id &gt;= 9500000 &amp;&amp; id &lt;= 9500084 &amp;&amp; ((id - 9500000) % 12) == 0) {      return (id - 9500000) / 12;    } else {      return id % 12;    }  }})</code></pre><pre><code>TIPS 这个做法相当于自定义 hash 表的 哈希函数。</code></pre><p>（2）适用场景</p><p>大量不同的 Key 被分配到了相同的 Task 造成该 Task 数据量过大。</p><p>（3）解决方案</p><p>使用自定义的 Partitioner 实现类代替默认的 HashPartitioner，尽量将所有不同的 Key 均匀分配到不同的 Task 中。</p><p>（4）优势</p><p>不影响原有的并行度设计。如果改变并行度，后续 Stage 的并行度也会默认改变，可能会影响后续 Stage。</p><p>（5）劣势</p><p>适用场景有限，只能将不同 Key 分散开，对于同一 Key 对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的 Partitioner，不够灵活。</p><p><strong>思路4. Reduce 端 Join 转化为 Map 端 Join</strong></p><p>通过 Spark 的 Broadcast 机制，将 Reduce 端 Join 转化为 Map 端 Join，这意味着 Spark 现在不需要跨节点做 shuffle 而是直接通过本地文件进行 join，从而完全消除 Shuffle 带来的数据倾斜。</p><p><img src="https://mmbiz.qpic.cn/mmbiz_png/4g5IMGibSxt7I00KzicX7Y9KLt7XnXKNwP4T4f0Fh2G77E7VTuRf58zZ050u9E48ykibGIiarraOFuXCZsMoiauNaKA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="img"></p><pre><code>from pyspark.sql.functions import broadcastresult = broadcast(A).join(B, [&quot;join_col&quot;], &quot;left&quot;)</code></pre><p>其中 A 是比较小的 dataframe 并且能够整个存放在 executor 内存中。</p><p>（1）适用场景</p><p>参与Join的一边数据集足够小，可被加载进 Driver 并通过 Broadcast 方法广播到各个 Executor 中。</p><p>（2）解决方案</p><p>在 Java/Scala 代码中将小数据集数据拉取到 Driver，然后通过 Broadcast 方案将小数据集的数据广播到各 Executor。或者在使用 SQL 前，将 Broadcast 的阈值调整得足够大，从而使 Broadcast 生效。进而将 Reduce Join 替换为 Map Join。</p><p>（3）优势</p><p>避免了 Shuffle，彻底消除了数据倾斜产生的条件，可极大提升性能。</p><p>（4）劣势</p><p>因为是先将小数据通过 Broadcase 发送到每个 executor 上，所以需要参与 Join 的一方数据集足够小，并且主要适用于 Join 的场景，不适合聚合的场景，适用条件有限。</p><pre><code>NOTES使用Spark SQL时需要通过 SET spark.sql.autoBroadcastJoinThreshold=104857600 将 Broadcast 的阈值设置得足够大，才会生效。</code></pre><p><strong>思路5. 拆分 join 再 union</strong></p><p>思路很简单，就是将一个 join 拆分成 倾斜数据集 Join 和 非倾斜数据集 Join，最后进行 union:</p><ol><li>对包含少数几个数据量过大的 key 的那个 RDD (假设是 leftRDD)，通过 sample 算子采样出一份样本来，然后统计一下每个 key 的数量，计算出来数据量最大的是哪几个 key。具体方法上面已经介绍过了，这里不赘述。</li><li>然后将这 k 个 key 对应的数据从 leftRDD 中单独过滤出来，并给每个 key 都打上 1~n 以内的随机数作为前缀，形成一个单独的 leftSkewRDD；而不会导致倾斜的大部分 key 形成另外一个 leftUnSkewRDD。</li><li>接着将需要 join 的另一个 rightRDD，也过滤出来那几个倾斜 key 并通过 flatMap 操作将该数据集中每条数据均转换为 n 条数据（这 n 条数据都按顺序附加一个 0~n 的前缀），形成单独的 rightSkewRDD；不会导致倾斜的大部分 key 也形成另外一个 rightUnSkewRDD。</li><li>现在将 leftSkewRDD 与 膨胀 n 倍的 rightSkewRDD 进行 join，且在 Join 过程中将随机前缀去掉，得到倾斜数据集的 Join 结果 skewedJoinRDD。注意到此时我们已经成功将原先相同的 key 打散成 n 份，分散到多个 task 中去进行 join 了。</li><li>对 leftUnSkewRDD 与 rightUnRDD 进行Join，得到 Join 结果 unskewedJoinRDD。</li><li>通过 union 算子将 skewedJoinRDD 与 unskewedJoinRDD 进行合并，从而得到完整的 Join 结果集。</li></ol><pre><code>TIPSrightRDD 与倾斜 Key 对应的部分数据，需要与随机前缀集 (1~n) 作笛卡尔乘积 (即将数据量扩大 n 倍），从而保证无论数据倾斜侧倾斜 Key 如何加前缀，都能与之正常 Join。skewRDD 的 join 并行度可以设置为 n * k (k 为 topSkewkey 的个数)。由于倾斜Key与非倾斜Key的操作完全独立，可并行进行。</code></pre><p>（1）适用场景</p><p>两张表都比较大，无法使用 Map 端 Join。其中一个 RDD 有少数几个 Key 的数据量过大，另外一个 RDD 的 Key 分布较为均匀。</p><p>（2）解决方案</p><p>将有数据倾斜的 RDD 中倾斜 Key 对应的数据集单独抽取出来加上随机前缀，另外一个 RDD 每条数据分别与随机前缀结合形成新的RDD（相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join并去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。</p><p>（3）优势</p><p>相对于 Map 则 Join，更能适应大数据集的 Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。</p><p>（4）劣势</p><p>如果倾斜 Key 非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜 Key 与非倾斜 Key 分开处理，需要扫描数据集两遍，增加了开销。</p><p><strong>思路6. 大表 key 加盐，小表扩大 N 倍 jion</strong></p><p>如果出现数据倾斜的 Key 比较多，上一种方法将这些大量的倾斜 Key 分拆出来，意义不大。此时更适合直接对存在数据倾斜的数据集全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集作笛卡尔乘积（即将数据量扩大N倍）。</p><p>其实就是上一个方法的特例或者简化。少了拆分，也就没有 union。</p><p>（1）适用场景</p><p>一个数据集存在的倾斜 Key 比较多，另外一个数据集数据分布比较均匀。</p><p>（2）优势</p><p>对大部分场景都适用，效果不错。</p><p>（3）劣势</p><p>需要将一个数据集整体扩大 N 倍，会增加资源消耗。</p><p><strong>思路7. map 端先局部聚合</strong></p><p>在 map 端加个 combiner 函数进行局部聚合。加上 combiner 相当于提前进行 reduce ,就会把一个 mapper 中的相同 key 进行聚合，减少 shuffle 过程中数据量 以及 reduce 端的计算量。这种方法可以有效的缓解数据倾斜问题，但是如果导致数据倾斜的 key 大量分布在不同的 mapper 的时候，这种方法就不是很有效了。</p><pre><code>TIPS 使用 reduceByKey 而不是 groupByKey。</code></pre><p><strong>思路8. 加盐局部聚合 + 去盐全局聚合</strong></p><p>这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个 key 都打上一个 1~n 的随机数，比如 3 以内的随机数，此时原先一样的 key 就变成不一样的了，比如 (hello, 1) (hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成 (1_hello, 1) (3_hello, 1) (2_hello, 1) (1_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行 reduceByKey 等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了 (1_hello, 2) (2_hello, 2) (3_hello, 1)。然后将各个 key 的前缀给去掉，就会变成 (hello, 2) (hello, 2) (hello, 1)，再次进行全局聚合操作，就可以得到最终结果了，比如 (hello, 5)。</p><pre><code>def antiSkew(): RDD[(String, Int)] = {    val SPLIT = &quot;-&quot;    val prefix = new Random().nextInt(10)    pairs.map(t =&gt; ( prefix + SPLIT + t._1, 1))        .reduceByKey((v1, v2) =&gt; v1 + v2)        .map(t =&gt; (t._1.split(SPLIT)(1), t2._2))        .reduceByKey((v1, v2) =&gt; v1 + v2)}</code></pre><p>不过进行两次 mapreduce，性能稍微比一次的差些。</p><h1 id="七、Hadoop-中的数据倾斜"><a href="#七、Hadoop-中的数据倾斜" class="headerlink" title="七、Hadoop 中的数据倾斜"></a>七、Hadoop 中的数据倾斜</h1><p>Hadoop 中直接贴近用户使用的是 Mapreduce 程序和 Hive 程序，虽说 Hive 最后也是用 MR 来执行（至少目前 Hive 内存计算并不普及），但是毕竟写的内容逻辑区别很大，一个是程序，一个是Sql，因此这里稍作区分。</p><p>Hadoop 中的数据倾斜主要表现在 ruduce 阶段卡在99.99%，一直99.99%不能结束。</p><p>这里如果详细的看日志或者和监控界面的话会发现:</p><ul><li>有一个多几个 reduce 卡住</li><li>各种 container报错 OOM</li><li>读写的数据量极大，至少远远超过其它正常的 reduce</li><li>伴随着数据倾斜，会出现任务被 kill 等各种诡异的表现。</li></ul><p><strong>经验:</strong> Hive的数据倾斜，一般都发生在 Sql 中 Group 和 On 上，而且和数据逻辑绑定比较深。</p><p><strong>优化方法</strong></p><p>这里列出来一些方法和思路，具体的参数和用法在官网看就行了。</p><ol><li><p>map join 方式</p></li><li><p>count distinct 的操作，先转成 group，再 count</p></li><li><p>参数调优</p><p>set hive.map.aggr=true</p><p>set hive.groupby.skewindata=true</p></li><li><p>left semi jion 的使用</p></li><li><p>设置 map 端输出、中间结果压缩。（不完全是解决数据倾斜的问题，但是减少了 IO 读写和网络传输，能提高很多效率）</p></li></ol><p>说明</p><p>hive.map.aggr=true: 在map中会做部分聚集操作，效率更高但需要更多的内存。</p><p>hive.groupby.skewindata=true: 数据倾斜时负载均衡，当选项设定为true，生成的查询计划会有两个MRJob。第一个MRJob 中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的GroupBy Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MRJob再根据预处理的数据结果按照GroupBy Key分布到Reduce中（这个过程可以保证相同的GroupBy Key被分布到同一个Reduce中），最后完成最终的聚合操作。</p><h1 id="八、参考文章"><a href="#八、参考文章" class="headerlink" title="八、参考文章"></a>八、参考文章</h1><ol><li><p>Spark性能优化之道——解决Spark数据倾斜（Data Skew）的N种姿势</p></li><li><p>漫谈千亿级数据优化实践：数据倾斜（纯干货）</p></li><li><p>解决spark中遇到的数据倾斜问题</p><p>转自 郑志彬  <a href="https://mp.weixin.qq.com/s?__biz=MzA3MDY0NTMxOQ==&amp;mid=2247487691&amp;idx=1&amp;sn=b7f21686709691e95b5595e26b6f3333&amp;chksm=9f38f3e3a84f7af50460a3a4e4ab9a71435b9adea3363a0c7bc2f86984c9282703ca446ec88a&amp;mpshare=1&amp;scene=2&amp;srcid=&amp;sharer_sharetime=1578103931014&amp;sharer_shareid=f41a021a947ec280f92fb7707bcfcf93&amp;from=timeline&amp;key=271532ef68e060ccfed6fc259401a68d650ad309d12586e5aa28cc7ecca969d84ac2d0fce59bd18e9b079f4ad93dd5fc39d70c69b8d8a9d3ed9f28489975758d13496c5a46f583a095d83df9396bc811&amp;ascene=14&amp;uin=MjYyNDA0NzcwMg%3D%3D&amp;devicetype=Windows+10&amp;version=62060833&amp;lang=zh_CN&amp;exportkey=AWacs9FCSvrjEYonPt%2B9%2Bw0%3D&amp;pass_ticket=e9JAYqu1tkk0S06jCjXJuPT4IsF8z9Gf6VU8VE7IztmSYEE1EB0JBf9Rkcl7M779" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzA3MDY0NTMxOQ==&amp;mid=2247487691&amp;idx=1&amp;sn=b7f21686709691e95b5595e26b6f3333&amp;chksm=9f38f3e3a84f7af50460a3a4e4ab9a71435b9adea3363a0c7bc2f86984c9282703ca446ec88a&amp;mpshare=1&amp;scene=2&amp;srcid=&amp;sharer_sharetime=1578103931014&amp;sharer_shareid=f41a021a947ec280f92fb7707bcfcf93&amp;from=timeline&amp;key=271532ef68e060ccfed6fc259401a68d650ad309d12586e5aa28cc7ecca969d84ac2d0fce59bd18e9b079f4ad93dd5fc39d70c69b8d8a9d3ed9f28489975758d13496c5a46f583a095d83df9396bc811&amp;ascene=14&amp;uin=MjYyNDA0NzcwMg%3D%3D&amp;devicetype=Windows+10&amp;version=62060833&amp;lang=zh_CN&amp;exportkey=AWacs9FCSvrjEYonPt%2B9%2Bw0%3D&amp;pass_ticket=e9JAYqu1tkk0S06jCjXJuPT4IsF8z9Gf6VU8VE7IztmSYEE1EB0JBf9Rkcl7M779</a> </p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一、什么是数据倾斜&quot;&gt;&lt;a href=&quot;#一、什么是数据倾斜&quot; class=&quot;headerlink&quot; title=&quot;一、什么是数据倾斜&quot;&gt;&lt;/a&gt;一、什么是数据倾斜&lt;/h1&gt;&lt;p&gt;对 Spark/Hadoop 这样的分布式大数据系统来讲，数据量大并不可怕，可怕的是数据倾斜。&lt;/p&gt;
&lt;p&gt;对于分布式系统而言，理想情况下，随着系统规模（节点数量）的增加，应用整体耗时线性下降。如果一台机器处理一批大量数据需要120分钟，当机器数量增加到3台时，理想的耗时为120 / 3 = 40分钟。但是，想做到分布式情况下每台机器执行时间是单机时的1 / N，就必须保证每台机器的任务量相等。不幸的是，很多时候，任务的分配是不均匀的，甚至不均匀到大部分任务被分配到个别机器上，其它大部分机器所分配的任务量只占总得的小部分。比如一台机器负责处理 80% 的任务，另外两台机器各处理 10% 的任务。&lt;/p&gt;
&lt;p&gt;『不患多而患不均』，这是分布式环境下最大的问题。意味着计算能力不是线性扩展的，而是存在短板效应: 一个 Stage 所耗费的时间，是由最慢的那个 Task 决定。&lt;/p&gt;
&lt;p&gt;由于同一个 Stage 内的所有 task 执行相同的计算，在排除不同计算节点计算能力差异的前提下，不同 task 之间耗时的差异主要由该 task 所处理的数据量决定。所以，要想发挥分布式系统并行计算的优势，就必须解决数据倾斜问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="https://weiye301.github.io/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://weiye301.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>sublime配置sftp</title>
    <link href="https://weiye301.github.io/2019/12/10/sublime%E9%85%8D%E7%BD%AEsftp/"/>
    <id>https://weiye301.github.io/2019/12/10/sublime%E9%85%8D%E7%BD%AEsftp/</id>
    <published>2019-12-10T03:04:12.000Z</published>
    <updated>2019-12-10T03:18:12.486Z</updated>
    
    <content type="html"><![CDATA[<p>记录一下sublime配置sftp</p><a id="more"></a><h1 id="1-选择安装sftp"><a href="#1-选择安装sftp" class="headerlink" title="1.选择安装sftp"></a>1.选择安装sftp</h1><p><img src="/images/image-20191210104713552.png" alt="image-20191210104713552"></p><p><img src="/images/image-20191210104730953.png" alt="image-20191210104730953"></p><p>然后搜索sftp点击安装即可，可能失败需要外网。</p><h1 id="2-配置服务器信息"><a href="#2-配置服务器信息" class="headerlink" title="2.配置服务器信息"></a>2.配置服务器信息</h1><p><img src="/images/image-20191210105944724.png" alt="image-20191210105944724"></p><p><img src="/images/image-20191210110105590.png" alt="image-20191210110105590"></p><p>改完保存！</p><h1 id="3-编辑服务器内的文件"><a href="#3-编辑服务器内的文件" class="headerlink" title="3.编辑服务器内的文件"></a>3.编辑服务器内的文件</h1><p><img src="/images/image-20191210110152341.png" alt="image-20191210110152341"></p><p>选择刚才配置的机器：</p><p><img src="/images/image-20191210110215050.png" alt="image-20191210110215050"></p><p>等待一会就连上了</p><p><img src="/images/image-20191210110320085.png" alt="image-20191210110320085"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一下sublime配置sftp&lt;/p&gt;
    
    </summary>
    
    
      <category term="使用" scheme="https://weiye301.github.io/categories/%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="Sublime" scheme="https://weiye301.github.io/tags/Sublime/"/>
    
  </entry>
  
  <entry>
    <title>安装k8s以及在k8s上部署hadoop</title>
    <link href="https://weiye301.github.io/2019/11/30/%E5%AE%89%E8%A3%85k8s%E4%BB%A5%E5%8F%8A%E5%9C%A8k8s%E4%B8%8A%E9%83%A8%E7%BD%B2hadoop/"/>
    <id>https://weiye301.github.io/2019/11/30/%E5%AE%89%E8%A3%85k8s%E4%BB%A5%E5%8F%8A%E5%9C%A8k8s%E4%B8%8A%E9%83%A8%E7%BD%B2hadoop/</id>
    <published>2019-11-30T08:55:55.000Z</published>
    <updated>2019-11-30T08:58:33.345Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kube-proxy开启ipvs的前置条件"><a href="#kube-proxy开启ipvs的前置条件" class="headerlink" title="kube-proxy开启ipvs的前置条件"></a>kube-proxy开启ipvs的前置条件</h2><pre><code class="shel">[root@hadoop102 ~]# modprobe br_netfilter[root@hadoop102 ~]# cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOF[root@hadoop102 ~]# chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4</code></pre><a id="more"></a><h2 id="安装-Docker-软件"><a href="#安装-Docker-软件" class="headerlink" title="安装 Docker 软件"></a>安装 Docker 软件</h2><pre><code class="shell">[root@hadoop102 ~]# yum install -y yum-utils device-mapper-persistent-data lvm2[root@hadoop102 ~]# yum-config-manager \  --add-repo \  http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo[root@hadoop102 ~]# vi /etc/yum.conf  [main]cachedir=/yumkeepcache=1debuglevel=2logfile=/var/log/yum.logexactarch=1obsoletes=1gpgcheck=1然后把yum文件夹放在/[root@hadoop102 ~]# chmod  777 /yum/[root@hadoop102 ~]# yum clean all[root@hadoop102 ~]# yum update -y &amp;&amp; yum install -y docker-ce## 创建 /etc/docker 目录[root@hadoop102 ~]# mkdir /etc/docker# 配置 daemon.[root@hadoop102 ~]# cat &gt; /etc/docker/daemon.json &lt;&lt;EOF{  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],  &quot;log-driver&quot;: &quot;json-file&quot;,  &quot;log-opts&quot;: {    &quot;max-size&quot;: &quot;100m&quot;  }}EOF[root@hadoop102 ~]# mkdir -p /etc/systemd/system/docker.service.d# 重启docker服务[root@hadoop102 ~]# systemctl daemon-reload &amp;&amp; systemctl restart docker &amp;&amp; systemctl enable docker</code></pre><h2 id="安装-Kubeadm-（主从配置）"><a href="#安装-Kubeadm-（主从配置）" class="headerlink" title="安装 Kubeadm （主从配置）"></a>安装 Kubeadm （主从配置）</h2><pre><code class="shel">[root@hadoop102 ~]# cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF[root@hadoop102 ~]# yum -y  install  kubeadm-1.15.1 kubectl-1.15.1 kubelet-1.15.1[root@hadoop102 ~]# systemctl enable kubelet.service</code></pre><h2 id="导入镜像（三台机器）"><a href="#导入镜像（三台机器）" class="headerlink" title="导入镜像（三台机器）"></a>导入镜像（三台机器）</h2><p>将kubeadm-basic.images.tar.gz镜像导入hadoop102，hadoop103，hadoop104</p><p>解压到当前目录（/root）</p><p>将load-images.sh脚本传进来，赋予执行权限</p><p>执行脚本</p><h2 id="初始化主节点"><a href="#初始化主节点" class="headerlink" title="初始化主节点"></a>初始化主节点</h2><pre><code class="she">[root@hadoop102 ~]# kubeadm config print init-defaults &gt; kubeadm-config.yaml    localAPIEndpoint:        advertiseAddress: 192.168.66.10    kubernetesVersion: v1.15.1    networking:      podSubnet: &quot;10.244.0.0/16&quot;      serviceSubnet: 10.96.0.0/12    ---    apiVersion: kubeproxy.config.k8s.io/v1alpha1    kind: KubeProxyConfiguration    featureGates:      SupportIPVSProxyMode: true    mode: ipvs[root@hadoop102 ~]# kubeadm init --config=kubeadm-config.yaml --experimental-upload-certs | tee kubeadm-init.log</code></pre><h2 id="加入主节点以及其余工作节点"><a href="#加入主节点以及其余工作节点" class="headerlink" title="加入主节点以及其余工作节点"></a>加入主节点以及其余工作节点</h2><pre><code class="shell">查看kubeadm-init.log可以发现下面的内容Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user:这里是加入主节点  mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:这里是加入子节点kubeadm join 192.168.79.102:6443 --token abcdef.0123456789abcdef \    --discovery-token-ca-cert-hash sha256:91ed76e9852dee956404390fd3d741a425de68c9456fc37228d8289b9c6ef548 执行上述两个地方的命令即可通过kubectl get node查看[root@hadoop102 ~]# kubectl get nodeNAME        STATUS   ROLES    AGE    VERSIONhadoop102   NotReady    master   13m    v1.15.1hadoop103   NotReady    &lt;none&gt;   104s   v1.15.1hadoop104   NotReady    &lt;none&gt;   102s   v1.15.1</code></pre><h2 id="部署网络"><a href="#部署网络" class="headerlink" title="部署网络"></a>部署网络</h2><pre><code class="shell">上一步执行完会发现所有机器的状态都为NotReady，这时候需要配置flannel网络[root@hadoop102 ~]# mkdir -p install-k8s/core[root@hadoop102 ~]# mv kubeadm-init.log kubeadm-config.yaml install-k8s/core[root@hadoop102 ~]# cd install-k8s/[root@hadoop102 install-k8s]# mkdir -p plugin/flannel[root@hadoop102 install-k8s]# cd plugin/flannel[root@hadoop102 flannel]# wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml --no-check-certificate[root@hadoop102 flannel]# kubectl create -f kube-flannel.yml查看节点状态：[root@hadoop102 flannel]# kubectl get nodeNAME        STATUS   ROLES    AGE    VERSIONhadoop102   Ready    master   13m    v1.15.1hadoop103   Ready    &lt;none&gt;   104s   v1.15.1hadoop104   Ready    &lt;none&gt;   102s   v1.15.1状态切换需要时间，发现还是NotReady需耐心等待</code></pre><h2 id="安装helm"><a href="#安装helm" class="headerlink" title="安装helm"></a>安装helm</h2><p>1.将helm-v2.13.1-linux-amd64.tar.gz上传到集群</p><p>2.解压</p><p>3.将解压后的目录中的helm拷贝到/usr/local/bin，并增加执行权限</p><pre><code>[root@hadoop102 ~]# cp linux-amd64/helm /usr/local/bin/[root@hadoop102 ~]# chmod a+x /usr/local/bin/helm</code></pre><h2 id="k8s部署hadoop"><a href="#k8s部署hadoop" class="headerlink" title="k8s部署hadoop"></a>k8s部署hadoop</h2><h4 id="1-创建一个文件夹存放hadoop相关文件"><a href="#1-创建一个文件夹存放hadoop相关文件" class="headerlink" title="1.创建一个文件夹存放hadoop相关文件"></a>1.创建一个文件夹存放hadoop相关文件</h4><pre><code>[root@hadoop102 ~]# mkdir hadoop-helm[root@hadoop102 ~]# cd hadoop-helm</code></pre><h4 id="2-在-https-hub-helm-sh-charts-stable-hadoop-上查看拉取hadoop镜像"><a href="#2-在-https-hub-helm-sh-charts-stable-hadoop-上查看拉取hadoop镜像" class="headerlink" title="2.在 https://hub.helm.sh/charts/stable/hadoop 上查看拉取hadoop镜像"></a>2.在 <a href="https://hub.helm.sh/charts/stable/hadoop" target="_blank" rel="noopener">https://hub.helm.sh/charts/stable/hadoop</a> 上查看拉取hadoop镜像</h4><pre><code>[root@hadoop102 hadoop-helm]# helm fetch stable/hadoop --version 1.1.1[root@hadoop102 hadoop-helm]# tar -zxvf hadoop-1.1.1.tgz[root@hadoop102 hadoop-helm]# cd hadoop修改文件，将格式化操作去除</code></pre><p>​    values.yaml文件里可以更改hadoop集群的属性，比如datanode个数，资源等</p><p><img src="C:%5CUsers%5Cwei%5CPictures%5CSnipaste_2019-11-30_08-39-10.png" alt="Snipaste_2019-11-30_08-39-10"></p><ol start="3"><li><h4 id="安装镜像"><a href="#安装镜像" class="headerlink" title="安装镜像"></a>安装镜像</h4><pre><code>[root@hadoop102 hadoop]# helm  install --name hadoop .查看启动信息，这里需要注意启动需要时间，等待一会[root@hadoop102 hadoop]# kubectl get podNAME                      READY   STATUS    RESTARTS   AGEhadoop-hadoop-hdfs-dn-0   1/1     Running   1          76shadoop-hadoop-hdfs-dn-1   1/1     Running   0          38shadoop-hadoop-hdfs-dn-2   1/1     Running   0          26shadoop-hadoop-hdfs-nn-0   1/1     Running   0          76shadoop-hadoop-yarn-nm-0   1/1     Running   1          76shadoop-hadoop-yarn-nm-1   1/1     Running   0          25shadoop-hadoop-yarn-rm-0   1/1     Running   0          76s查看更详细的信息：[root@hadoop102 hadoop]# kubectl get pod -o wideNAME                      READY   STATUS    RESTARTS   AGE     IP            NODE        NOMINATED NODE   READINESS GATEShadoop-hadoop-hdfs-dn-0   1/1     Running   1          5m39s   10.244.1.28   hadoop103   &lt;none&gt;           &lt;none&gt;hadoop-hadoop-hdfs-dn-1   1/1     Running   0          5m1s    10.244.2.31   hadoop104   &lt;none&gt;           &lt;none&gt;hadoop-hadoop-hdfs-dn-2   1/1     Running   0          4m49s   10.244.2.32   hadoop104   &lt;none&gt;           &lt;none&gt;hadoop-hadoop-hdfs-nn-0   1/1     Running   0          5m39s   10.244.2.29   hadoop104   &lt;none&gt;           &lt;none&gt;hadoop-hadoop-yarn-nm-0   1/1     Running   1          5m39s   10.244.1.29   hadoop103   &lt;none&gt;           &lt;none&gt;hadoop-hadoop-yarn-nm-1   1/1     Running   0          4m48s   10.244.2.33   hadoop104   &lt;none&gt;           &lt;none&gt;hadoop-hadoop-yarn-rm-0   1/1     Running   0          5m39s   10.244.2.30   hadoop104   &lt;none&gt;           &lt;none&gt;如果某个pod迟迟启动不了，比如hadoop-hadoop-yarn-rm-0，可以通过下面的命令查看kubectl describe  pod hadoop-hadoop-yarn-rm-0一般是镜像下载失败，此时我们可以去下载成功的节点将镜像导出，然后在hadoop-hadoop-yarn-rm-0运行的节点（上面是hadoop104）导入。（这里只是举例子，不是说一定是镜像问题）先：docker images 查看是否有镜像在有镜像的节点导出镜像：[root@hadoop102 hadoop]# docker save -o hadoop.tar danisla/hadoop:2.9.0上面命令的意思是将danisla/hadoop:2.9.0镜像导出到本地，并起名为hadoop.tar将hadoop.tar上传到hadoop-hadoop-yarn-rm-0所在节点，我这里是hadoop104然后在hadoop-hadoop-yarn-rm-0所在节点导入：[root@hadoop104 ~]# docker load -i hadoop.tar之后去主节点查看状态即可</code></pre><ol start="4"><li><h4 id="暴露端口"><a href="#暴露端口" class="headerlink" title="暴露端口"></a>暴露端口</h4><p>此时hadoop运行无误，但是此时我们不能访问，因为hadoop的端口是pod内部共享的，我们在外面用不了，所以此时需要将我们用到的端口暴露出来。</p><h5 id="4-1-50070"><a href="#4-1-50070" class="headerlink" title="4.1 50070"></a>4.1 50070</h5><h6 id="（1）创建svc文件"><a href="#（1）创建svc文件" class="headerlink" title="（1）创建svc文件"></a>（1）创建svc文件</h6><ul><li><pre><code class="yaml">[root@hadoop102 hadoop]# vim nnport.yamlapiVersion: v1kind: Servicemetadata:  name: nnport  labels:    name: nnportspec:  type: NodePort      #这里代表是NodePort类型的,暴露端口需要此类型  ports:  - port: 50070        #这里的端口就是要暴露的,供内部访问。    targetPort: 50070  #端口一定要和暴露出来的端口对应    protocol: TCP    nodePort: 30070   # 所有的节点都会开放此端口，此端口供外部调用，需要大于30000。  selector:    app: hadoop    component: hdfs-nn    release: hadoop</code></pre><p>上述文件的selector要和我们此时的环境对应上，可以通过下面命令查看：</p><pre><code>[root@hadoop102 hadoop]# kubectl  edit svc hadoop-hadoop-hdfs-nn</code></pre></li></ul></li></ol></li></ol><pre><code>  ![Snipaste_2019-11-30_09-03-44](C:\Users\wei\Pictures\Snipaste_2019-11-30_09-03-44.png)  ###### （2）开启端口  ```  [root@hadoop102 hadoop]# kubectl  apply -f nnport.yaml  ```  查看：  ```  [root@hadoop102 hadoop]# kubectl  get svc  nnport  NodePort   10.97.175.201   &lt;none&gt;    50070:30070/TCP      63s  ```  ###### （3）访问webui  ```  先进nn容器内，格式化namenode以及启动namenode  [root@hadoop102 hadoop]# kubectl exec -it hadoop-hadoop-hdfs-nn-0 /bin/bash  root@hadoop-hadoop-hdfs-nn-0:/usr/local/hadoop-2.9.0# hdfs namenode -format  root@hadoop-hadoop-hdfs-nn-0:/usr/local/hadoop-2.9.0# sbin/hadoop-demon.sh start namenode  然后进dn容器，启动datanode  [root@hadoop102 hadoop]# kubectl exec -it hadoop-hadoop-hdfs-dn-0 /bin/bash  root@hadoop-hadoop-hdfs-dn-0:/usr/local/hadoop-2.9.0# sbin/hadoop-demon.sh start datanode  ```  ![Snipaste_2019-11-30_09-29-38](C:\Users\wei\Pictures\Snipaste_2019-11-30_09-29-38.png)  ##### 4.2 8088  ###### （1）创建svc文件(与50070同理)  ```yaml  [root@hadoop102 hadoop]# vim rmport.yaml  apiVersion: v1  kind: Service  metadata:    name: rmport    labels:      name: rmport  spec:    type: NodePort      #这里代表是NodePort类型的    ports:    - port: 8088          targetPort: 8088       protocol: TCP      nodePort: 30088   # 所有的节点都会开放此端口，此端口供外部调用。    selector:   app: hadoop      component: yarn-rm   release: hadoop  ```  ###### （2）开启端口  ```  [root@hadoop102 hadoop]# kubectl create -f rmport.yaml   ```  ###### （3）访问webui  ![Snipaste_2019-11-30_09-28-07](C:\Users\wei\Pictures\Snipaste_2019-11-30_09-28-07.png)</code></pre><h5 id="4-3-测试"><a href="#4-3-测试" class="headerlink" title="4.3 测试"></a>4.3 测试</h5><p>先进入容器内部：</p><pre><code>[root@hadoop102 hadoop]# kubectl exec -it hadoop-hadoop-hdfs-nn-0 /bin/bashroot@hadoop-hadoop-hdfs-nn-0:/usr/local/hadoop-2.9.0# hadoop fs -mkdir /testroot@hadoop-hadoop-hdfs-nn-0:/usr/local/hadoop-2.9.0# hadoop fs -put README.txt /test</code></pre><p><img src="C:%5CUsers%5Cwei%5CPictures%5CSnipaste_2019-11-30_09-40-05.png" alt="Snipaste_2019-11-30_09-40-05"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;kube-proxy开启ipvs的前置条件&quot;&gt;&lt;a href=&quot;#kube-proxy开启ipvs的前置条件&quot; class=&quot;headerlink&quot; title=&quot;kube-proxy开启ipvs的前置条件&quot;&gt;&lt;/a&gt;kube-proxy开启ipvs的前置条件&lt;/h2&gt;&lt;pre&gt;&lt;code class=&quot;shel&quot;&gt;[root@hadoop102 ~]# modprobe br_netfilter

[root@hadoop102 ~]# cat &amp;gt; /etc/sysconfig/modules/ipvs.modules &amp;lt;&amp;lt;EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
[root@hadoop102 ~]# chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; bash /etc/sysconfig/modules/ipvs.modules &amp;amp;&amp;amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
    
      <category term="K8S" scheme="https://weiye301.github.io/categories/K8S/"/>
    
    
      <category term="BigData" scheme="https://weiye301.github.io/tags/BigData/"/>
    
      <category term="K8S" scheme="https://weiye301.github.io/tags/K8S/"/>
    
      <category term="Docker" scheme="https://weiye301.github.io/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>hue调度oozie，oozie再调度sqoop遇到的问题</title>
    <link href="https://weiye301.github.io/2019/07/15/hue%E8%B0%83%E5%BA%A6oozie%EF%BC%8Coozie%E5%86%8D%E8%B0%83%E5%BA%A6sqoop%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>https://weiye301.github.io/2019/07/15/hue%E8%B0%83%E5%BA%A6oozie%EF%BC%8Coozie%E5%86%8D%E8%B0%83%E5%BA%A6sqoop%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2019-07-15T08:15:26.000Z</published>
    <updated>2019-11-20T02:40:37.121Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-一直心跳，不执行任务"><a href="#1-一直心跳，不执行任务" class="headerlink" title="1.一直心跳，不执行任务"></a>1.一直心跳，不执行任务</h2><a id="more"></a><p>这是因为资源不足，下图所示的容器内存多给点<br><img src="https://img-blog.csdnimg.cn/2019071516075349.png" alt="调节资源"></p><h2 id="2-执行sqoop任务失败"><a href="#2-执行sqoop任务失败" class="headerlink" title="2.执行sqoop任务失败"></a>2.执行sqoop任务失败</h2><p><img src="https://img-blog.csdnimg.cn/20190715160842542.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="sqoop导入失败"><br>这个我一开始是admin用户登录，后来突发奇想换个用户试试，然后创建了一个用户名和密码都是yarn的用户，重新登录，重新写workflow，然后执行就成功了，原因暂时还不知道。<br><img src="https://img-blog.csdnimg.cn/20190715161037845.png" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-一直心跳，不执行任务&quot;&gt;&lt;a href=&quot;#1-一直心跳，不执行任务&quot; class=&quot;headerlink&quot; title=&quot;1.一直心跳，不执行任务&quot;&gt;&lt;/a&gt;1.一直心跳，不执行任务&lt;/h2&gt;
    
    </summary>
    
    
      <category term="问题" scheme="https://weiye301.github.io/categories/%E9%97%AE%E9%A2%98/"/>
    
    
      <category term="hue" scheme="https://weiye301.github.io/tags/hue/"/>
    
      <category term="oozie" scheme="https://weiye301.github.io/tags/oozie/"/>
    
  </entry>
  
  <entry>
    <title>ssh突然失效</title>
    <link href="https://weiye301.github.io/2019/07/15/ssh%E7%AA%81%E7%84%B6%E5%A4%B1%E6%95%88/"/>
    <id>https://weiye301.github.io/2019/07/15/ssh%E7%AA%81%E7%84%B6%E5%A4%B1%E6%95%88/</id>
    <published>2019-07-15T06:33:12.000Z</published>
    <updated>2019-11-20T02:40:43.572Z</updated>
    
    <content type="html"><![CDATA[<p>遇到很多次ssh突然某台机器失效：<br>    解决方法如下：</p><a id="more"></a><pre><code>    首先查看/var/log/sec..（后面记不清了）日志，发现权限不对    修改用户家目录权限：chmod 700 /home/user    如果还不行继续修改.ssh权限：chmod 600 /home/user/.ssh</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;遇到很多次ssh突然某台机器失效：&lt;br&gt;    解决方法如下：&lt;/p&gt;
    
    </summary>
    
    
      <category term="问题" scheme="https://weiye301.github.io/categories/%E9%97%AE%E9%A2%98/"/>
    
    
      <category term="Linux" scheme="https://weiye301.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>hexo文章标签分类等设置</title>
    <link href="https://weiye301.github.io/2019/06/16/hexo%E6%96%87%E7%AB%A0%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E7%AD%89%E8%AE%BE%E7%BD%AE/"/>
    <id>https://weiye301.github.io/2019/06/16/hexo%E6%96%87%E7%AB%A0%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E7%AD%89%E8%AE%BE%E7%BD%AE/</id>
    <published>2019-06-16T03:55:26.000Z</published>
    <updated>2019-11-20T02:40:25.378Z</updated>
    
    <content type="html"><![CDATA[<h2 id="文章基本设置"><a href="#文章基本设置" class="headerlink" title="文章基本设置"></a>文章基本设置</h2><hr><p>title: test<br>date: 2019-05-08 11:51:54</p><a id="more"></a><p>tags:<br>    - clickhouse #加空格<br>comments: true #是否可评论<br>categories: “数据库” #分类<br>index_img: /img/example.jpg #设置文章预览的图片</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;文章基本设置&quot;&gt;&lt;a href=&quot;#文章基本设置&quot; class=&quot;headerlink&quot; title=&quot;文章基本设置&quot;&gt;&lt;/a&gt;文章基本设置&lt;/h2&gt;&lt;hr&gt;
&lt;p&gt;title: test&lt;br&gt;date: 2019-05-08 11:51:54&lt;/p&gt;
    
    </summary>
    
    
      <category term="使用" scheme="https://weiye301.github.io/categories/%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="hexo" scheme="https://weiye301.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>Centos6.8安装Ambari</title>
    <link href="https://weiye301.github.io/2019/05/16/Centos6-8%E5%AE%89%E8%A3%85Ambari/"/>
    <id>https://weiye301.github.io/2019/05/16/Centos6-8%E5%AE%89%E8%A3%85Ambari/</id>
    <published>2019-05-16T06:33:48.000Z</published>
    <updated>2019-12-27T03:50:19.119Z</updated>
    
    <content type="html"><![CDATA[<h2 id="第1章-Ambari简述"><a href="#第1章-Ambari简述" class="headerlink" title="第1章 Ambari简述"></a>第1章 Ambari简述</h2><p><strong>1.1 什么是Ambari</strong><br>Apache Ambari项目旨在通过开发用于配置，管理和监控Apache Hadoop集群的软件来简化Hadoop管理。Ambari提供了一个直观，易用的Hadoop管理Web UI。</p><a id="more"></a><p><strong>1.2 Ambari的功能</strong><br>Ambari提供了跨任意数量的主机安装Hadoop服务的分步向导。<br>Ambari处理群集的Hadoop服务配置。<br>Ambari提供集中管理，用于在整个集群中启动，停止和重新配置Hadoop服务。<br>Ambari提供了一个仪表板，用于监控Hadoop集群的运行状况和状态。<br>Ambari利用Ambari指标系统进行指标收集。<br>Ambari利用Ambari Alert Framework进行系统警报，并在需要您注意时通知您（例如，节点出现故障，剩余磁盘空间不足等）。</p><h2 id="第2章-环境准备"><a href="#第2章-环境准备" class="headerlink" title="第2章 环境准备"></a>第2章 环境准备</h2><p><strong>以下操作三台机器都需要进行</strong><br><strong>2.1 虚拟机准备</strong><br>克隆三台虚拟机（hadoop102、hadoop103、hadoop104），配置好对应主机的网络IP、主机名称、关闭防火墙。</p><pre><code>[root@hadoop102 ~]# chkconfig iptables off[root@hadoop102 ~]# service iptables stop[root@hadoop102 ~]# chkconfig --list iptablesiptables        0:关闭  1:关闭  2:关闭  3:关闭  4:关闭  5:关闭  6:关闭</code></pre><p><strong>2.2 关闭SELINUX</strong></p><pre><code>[root@hadoop102 ~]# vim /etc/sysconfig/selinux将SELINUX=enforcing改为SELINUX=disabled</code></pre><p>执行该命令后重启机器生效</p><p><strong>2.3 安装JDK</strong><br>1）在hadoop102的/opt目录下创建module和software文件夹</p><pre><code>[root@hadoop102 opt]# mkdir module[root@hadoop102 opt]# mkdir software</code></pre><p>2）用SecureCRT将jdk-8u144-linux-x64.tar.gz导入到hadoop102的/opt/software目录下<br>3）在Linux系统下的opt目录中查看软件包是否导入成功</p><pre><code>[root@hadoop102 software]$ lsjdk-8u144-linux-x64.tar.gz</code></pre><p>4）解压JDK到/opt/module目录下</p><pre><code>[root@hadoop102 software]$ tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/</code></pre><p>5）配置JDK环境变量<br>    （1）先获取JDK路径</p><pre><code>[root@hadoop102 jdk1.8.0_144]$ pwd    /opt/module/jdk1.8.0_144</code></pre><p>（2）打开/etc/profile文件</p><pre><code>[root@hadoop102 software]$ vi /etc/profile</code></pre><p>在profile文件末尾添加JDK路径</p><pre><code>#JAVA_HOMEexport JAVA_HOME=/opt/module/jdk1.8.0_144export PATH=$PATH:$JAVA_HOME/bin</code></pre><p>（3）保存后退出</p><pre><code>:wq</code></pre><p>（4）让修改后的文件生效</p><pre><code>[root@hadoop102 jdk1.8.0_144]$ source /etc/profile</code></pre><p>6）测试JDK是否安装成功</p><pre><code>[root@hadoop102 jdk1.8.0_144]# java -versionjava version &quot;1.8.0_144&quot;</code></pre><p>7）将hadoop102中的JDK和环境变量分发到hadoop103、hadoop104两台主机</p><pre><code>[root@hadoop102 opt]# xsync /opt/module/[root@hadoop102 opt]# xsync /etc/profilexysnc 是自定义的分发脚本</code></pre><p>分别在hadoop103、hadoop104上source一下</p><pre><code>[root@hadoop103 ~]$ source /etc/profile[root@hadoop104 ~]# source /etc/profile</code></pre><p><strong>2.4 SSH免密登录</strong><br>配置hadoop102对hadoop102、hadoop103、hadoop104三台主机的免密登陆。<br>（1）生成公钥和私钥：</p><pre><code>[root@hadoop102 .ssh]$ ssh-keygen -t rsa</code></pre><p>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）<br>（2）将公钥拷贝到要免密登录的目标机器上</p><pre><code>[root@hadoop102 .ssh]$ ssh-copy-id hadoop102[root@hadoop102 .ssh]$ ssh-copy-id hadoop103[root@hadoop102 .ssh]$ ssh-copy-id hadoop104</code></pre><p><strong>2.5 修改yum源为阿里云镜像</strong></p><pre><code>[root@hadoop102 yum.repos.d]# mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bk[root@hadoop102 yum.repos.d]# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo[root@hadoop102 yum.repos.d]# yum makecache</code></pre><p><strong>2.6 安装ntp</strong></p><pre><code>[root@hadoop102 ~]# yum install -y ntp[root@hadoop102 ~]# chkconfig --list ntpd[root@hadoop102 ~]# chkconfig ntpd on[root@hadoop102 ~]# service ntpd start</code></pre><p><strong>2.7 关闭Linux的THP服务</strong><br>如果不关闭transparent_hugepage，HDFS会因为这个性能严重受影响。<br>关闭transparent_hugepage方法是：</p><pre><code>[root@hadoop102 ~]# vim /etc/grub.conf 添加 transparent_hugepage=never[root@hadoop102 ~]# vim /etc/rc.local添加：if test -f /sys/kernel/mm/transparent_hugepage/defrag; then  echo never &gt; /sys/kernel/mm/transparent_hugepage/defragfiif test -f /sys/kernel/mm/transparent_hugepage/enabled; then  echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledfiexit 0</code></pre><p>重启之后，用下面的命令检查：</p><pre><code>[root@hadoop102 yum.repos.d]# cat /sys/kernel/mm/redhat_transparent_hugepage/enabledalways madvise [never]</code></pre><p>有 [never]则表示THP被禁用<br><strong>2.8 配置UMASK</strong><br>设定用户所创建目录的初始权限</p><pre><code>[root@hadoop102 ~]# umask 0022</code></pre><p><strong>2.9 禁止离线更新</strong></p><pre><code>vim /etc/yum/pluginconf.d/refresh-packagekit.conf修改：enabled=0</code></pre><h2 id="第3章-安装Ambari集群"><a href="#第3章-安装Ambari集群" class="headerlink" title="第3章 安装Ambari集群"></a>第3章 安装Ambari集群</h2><p><strong>以下操作主节点操作即可</strong><br><strong>3.1 制作本地源</strong><br>制作本地源是因为在线安装Ambari太慢。制作本地源只需在主节点上进行。<br>3.1.1 配置HTTP 服务<br>配置HTTP 服务到系统层使其随系统自动启动</p><pre><code>[root@hadoop102 ~]# chkconfig httpd on[root@hadoop102 ~]# service httpd start</code></pre><p>3.1.2 安装工具<br>安装本地源制作相关工具</p><pre><code>[root@hadoop102 ~]# yum install yum-utils createrepo yum-plugin-priorities -y[root@hadoop102 ~]# vim /etc/yum/pluginconf.d/priorities.conf添加gpgcheck=0</code></pre><p>3.1.3 将下载的3个tar包解压</p><pre><code>[root@hadoop102 software]# tar -zxvf /opt/software/ambari-2.5.0.3-centos6.tar.gz -C /var/www/html/[root@hadoop102 software]mkdir /var/www/html/hdp[root@hadoop102 software]# tar -zxvf /opt/software/HDP-2.6.0.3-centos6-rpm.tar.gz -C /var/www/html/hdp[root@hadoop102 software]# tar -zxvf /opt/software/HDP-UTILS-1.1.0.21-centos6.tar.gz -C /var/www/html/hdp</code></pre><p>3.1.4 创建本地源</p><pre><code>[root@hadoop102 software]# cd /var/www/html/[root@hadoop102 html]# createrepo  ./修改ambari.repo，配置为本地源[root@hadoop102 html]# vim /etc/yum.repos.d/ambari.repo#VERSION_NUMBER=2.6.1.5-3[ambari-2.6.1.5]name=ambari Version - ambari-2.6.1.5baseurl=http://hadoop102/ambari/centos6/gpgcheck=0gpgkey=http://hadoop102/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1修改hdp-util.repo，配置为本地源[root@hadoop102 hdp]# vim /var/www/html/hdp/hdp-util.repo[HDP-UTILS-1.1.0.21]name=Hortonworks Data Platform Version - HDP-UTILS-1.1.0.21baseurl=http://hadoop102/hdp/gpgcheck=0enabled=1priority=1修改hdp.repo，配置为本地源[root@hadoop102 centos6]# vim /var/www/html/hdp/HDP/centos6/hdp.repo#VERSION_NUMBER=2.6.0.3-8[HDP-2.6.0.3]name=HDP Version - HDP-2.6.0.3baseurl=http://hadoop102/hdp/HDP/centos6/gpgcheck=0gpgkey=http://hadoop102/hdp/HDP/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1[HDP-UTILS-1.1.0.21]name=HDP-UTILS Version - HDP-UTILS-1.1.0.21baseurl=http://hadoop102/hdp/gpgcheck=0gpgkey=http://hadoop102/hdp/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1</code></pre><p>3.1.5 将Ambari存储库文件下载到安装主机上的目录中</p><pre><code>[root@hadoop102 yum.repos.d]# wget -nv http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.5/ambari.repo -O /etc/yum.repos.d/ambari.repo[root@hadoop102 ~]# yum clean all[root@hadoop102 ~]# yum makecache查看是否有Ambari[root@hadoop102 ~]# yum repolist</code></pre><p>查看Ambari 与 HDP 资源的资源库。<br>也可以打开浏览器查看一下：<br><a href="http://hadoop102/ambari/centos6/" target="_blank" rel="noopener">http://hadoop102/ambari/centos6/</a><br><a href="http://hadoop102/hdp/HDP/centos6/" target="_blank" rel="noopener">http://hadoop102/hdp/HDP/centos6/</a><br><a href="http://hadoop102/hdp/" target="_blank" rel="noopener">http://hadoop102/hdp/</a><br><strong>3.2 安装MySQL</strong><br>Ambari使用的默认数据库是PostgreSQL，用于存储安装元数据，可以使用自己安装MySQL数据库作为Ambari元数据库。<br><strong>注意：一定要用root用户操作如下步骤；先卸载MySQL再安装</strong><br>1）安装包准备<br>    （1）查看MySQL是否安装</p><pre><code>[root@hadoop102 桌面]# rpm -qa|grep mysql    mysql-libs-5.1.73-7.el6.x86_64</code></pre><p>（2）如果安装了MySQL，就先卸载</p><pre><code>[root@hadoop102 桌面]# rpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64</code></pre><p>（3）解压mysql-libs.zip文件到当前目录</p><pre><code>[root@hadoop102 software]# unzip mysql-libs.zip[root@hadoop102 software]# lsmysql-libs.zipmysql-libs</code></pre><p>（4）进入到mysql-libs文件夹下</p><pre><code> [root@hadoop102 mysql-libs]# ll总用量 76048-rw-r--r--. 1 root root 18509960 3月  26 2015 MySQL-client-5.6.24-1.el6.x86_64.rpm-rw-r--r--. 1 root root  3575135 12月  1 2013 mysql-connector-java-5.1.27.tar.gz-rw-r--r--. 1 root root 55782196 3月  26 2015 MySQL-server-5.6.24-1.el6.x86_64.rpm</code></pre><p>2）安装MySQL服务器<br>（1）安装MySQL服务端</p><pre><code>[root@hadoop102 mysql-libs]# rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</code></pre><p>（2）查看产生的随机密码</p><pre><code>[root@hadoop102 mysql-libs]# cat /root/.mysql_secretOEXaQuS8IWkG19Xs</code></pre><p>（3）查看MySQL状态</p><pre><code>[root@hadoop102 mysql-libs]# service mysql status</code></pre><p>（4）启动MySQL</p><pre><code>[root@hadoop102 mysql-libs]# service mysql start</code></pre><p>3）安装MySQL客户端<br>（1）安装MySQL客户端</p><pre><code>[root@hadoop102 mysql-libs]# rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm</code></pre><p>（2）链接MySQL </p><pre><code>[root@hadoop102 mysql-libs]# mysql -uroot -pOEXaQuS8IWkG19Xs</code></pre><p>（3）修改密码</p><pre><code>mysql&gt;SET PASSWORD=PASSWORD(&#39;000000&#39;);</code></pre><p>（4）退出MySQL</p><pre><code>mysql&gt;exit</code></pre><p>4）MySQL中user表中主机配置<br>配置只要是root用户+密码，在任何主机上都能登录MySQL数据库。<br>（1）进入MySQL</p><pre><code>[root@hadoop102 mysql-libs]# mysql -uroot -p000000</code></pre><p>（2）显示数据库</p><pre><code>mysql&gt;show databases;</code></pre><p>（3）使用MySQL数据库</p><pre><code>mysql&gt;use mysql;</code></pre><p>（4）展示MySQL数据库中的所有表</p><pre><code>mysql&gt;show tables;</code></pre><p>（5）展示user表的结构</p><pre><code>mysql&gt;desc user;</code></pre><p>（6）查询user表</p><pre><code>mysql&gt;select User, Host, Password from user;</code></pre><p>（7）修改user表，把Host表内容修改为%</p><pre><code>mysql&gt;update user set host=&#39;%&#39; where host=&#39;localhost&#39;;</code></pre><p>（8）删除root用户的其他host</p><pre><code>mysql&gt;delete from user where Host=&#39;hadoop102&#39;;delete from user where Host=&#39;127.0.0.1&#39;;delete from user where Host=&#39;::1&#39;;</code></pre><p>（9）刷新</p><pre><code>mysql&gt;flush privileges;</code></pre><p>（10）退出</p><pre><code>mysql&gt;quit;</code></pre><p>3.3 安装Ambari<br>1）安装ambari-server</p><pre><code> [root@hadoop102 hdp]# yum install ambari-server</code></pre><p>2） 拷贝mysql驱动<br>将mysql-connector-java.jar复制到/usr/share/java目录下并改名为mysql-connector-java.jar</p><pre><code>[root@hadoop102 hdp]# mkdir /usr/share/java[root@hadoop102 hdp]# cp /opt/software/mysql-libs/mysql-connector-java-5.1.27/mysql-connector-java-5.1.27-bin.jar /usr/share/java/mysql-connector-java.jar</code></pre><p>将mysql-connector-java.jar复制到/var/lib/ambari-server/resources目录下并改名为mysql-jdbc-driver.jar</p><pre><code>[root@hadoop102 hdp]# cp /usr/share/java/mysql-connector-java.jar /var/lib/ambari-server/resources/mysql-jdbc-driver.jar</code></pre><p>修改ambari.properties文件</p><pre><code>[root@hadoop102 hdp]#vim     /etc/ambari-server/conf/ambari.properties</code></pre><p>添加</p><pre><code>server.jdbc.driver.path=/usr/share/java/mysql-connector-java.jar</code></pre><h2 id="3-4-在MySQL中创建数据库"><a href="#3-4-在MySQL中创建数据库" class="headerlink" title="3.4 在MySQL中创建数据库"></a>3.4 在MySQL中创建数据库</h2><p>1）创建ambari库</p><pre><code>[root@hadoop102 hdp]# mysql -u root -p000000 mysql &gt;create database ambari; </code></pre><p>2）使用Ambari自带脚本创建表</p><pre><code>mysql &gt;use ambari; mysql&gt;source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql;</code></pre><p>3）赋予用户root权限：</p><pre><code>mysql&gt; grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;000000&#39;;</code></pre><p>4）刷新</p><pre><code>mysql&gt; flush privileges;</code></pre><h2 id="3-5-配置Ambari"><a href="#3-5-配置Ambari" class="headerlink" title="3.5 配置Ambari"></a>3.5 配置Ambari</h2><p>执行</p><pre><code>[root@hadoop102 hdp]# ambari-server setup</code></pre><p>下面是配置执行流程，按照提示操作</p><p>1） 提示是否自定义设置。输入：y </p><p><code>Customize user account for ambari-server daemon [y/n] (n)? y</code></p><p> 2）ambari-server 账号。 </p><pre><code>Enter user account for ambari-server daemon (root):</code></pre><p> 如果直接回车就是默认选择root用户 </p><p>3）检查防火墙是否关闭 </p><p><code>Adjusting ambari-server permissions and ownership... Checking firewall... WARNING: iptables is running. Confirm the necessary Ambari ports are accessible. Refer to the Ambari documentation for more details on ports. OK to continue [y/n] (y)?</code> </p><p>直接回车 </p><p>4）设置JDK。输入：3 </p><pre><code>Checking JDK... Do you want to change Oracle JDK [y/n] (n)? y [1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8 [2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7 [3] Custom JDK ============================================================================== Enter choice (1): 3 </code></pre><p>如果上面选择3自定义JDK,则需要设置JAVA_HOME。输入：/opt/module/jdk1.8.0_144 <code>WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts. WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts. Path to JAVA_HOME: /opt/module/jdk1.8.0_144 Validating JDK on Ambari Server...done. Completing setup...</code> </p><p>5）数据库配置。选择：y </p><pre><code>Configuring database... Enter advanced database configuration [y/n] (n)? y </code></pre><p>6）选择数据库类型。输入：3 </p><pre><code>Configuring database... ============================================================================== Choose one of the following options: [1] - PostgreSQL (Embedded) [2] - Oracle [3] - MySQL [4] - PostgreSQL [5] - Microsoft SQL Server (Tech Preview) [6] - SQL Anywhere ==============================================================================     Enter choice (3): 3 </code></pre><p>7）设置数据库的具体配置信息，根据实际情况输入，如果和括号内相同，则可以直接回车。如果想重命名，就输入。 </p><pre><code>Hostname (localhost):hadoop102 Port (3306): Database name (ambari): Username (ambari):root Enter Database Password (bigdata): Re-Enter password: </code></pre><p>8）将Ambari数据库脚本导入到数据库 </p><p><code>WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql Proceed with configuring remote database connection properties [y/n] (y)?</code> </p><p>如果使用自己定义的数据库，必须在启动Ambari服务之前导入Ambari的sql脚本。 </p><p><strong>3.6 启动Ambari</strong> </p><p>启动命令为： <code>ambari-server start</code> 停止命令为： <code>ambari-server stop</code> </p><h1 id="第4章-HDP集群部署"><a href="#第4章-HDP集群部署" class="headerlink" title="第4章 HDP集群部署"></a>第4章 HDP集群部署</h1><p>4.1 集群搭建 </p><p>4.1.1进入登录页面 浏览器输入<a href="http://hadoop102:8080/" target="_blank" rel="noopener">http://hadoop102:8080/</a> </p><p>默认管理员账目密码：admin</p><p><img src="https://img-blog.csdnimg.cn/20190509172907284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>4.1.2 点击Launch Install Wizard<br> <img src="https://img-blog.csdnimg.cn/20190509172915630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>4.1.3 设置集群名称<br> <img src="https://img-blog.csdnimg.cn/20190509172921642.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>4.1.4选择版本和存储库<br> <img src="https://img-blog.csdnimg.cn/20190509172930677.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>4.1.5 填写本地库地址<br>在redhat6后面分别填写<br><a href="http://hadoop102/hdp/HDP/centos6/" target="_blank" rel="noopener">http://hadoop102/hdp/HDP/centos6/</a><br><a href="http://hadoop102/hdp/" target="_blank" rel="noopener">http://hadoop102/hdp/</a></p><p> <img src="https://img-blog.csdnimg.cn/20190509172937156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>4.1.6 填写主机地址以及主节点的id.rsa文件<br> <img src="https://img-blog.csdnimg.cn/20190509172941791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>4.1.7 等待安装<br> <img src="https://img-blog.csdnimg.cn/20190509172948628.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>4.1.8 选择服务<br> <img src="https://img-blog.csdnimg.cn/20190509172954631.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>4.1.9 选择每台机器的角色<br> <img src="https://img-blog.csdnimg.cn/20190509172959265.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>4.1.10 设置从节点<br> <img src="https://img-blog.csdnimg.cn/20190509173005164.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>4.1.11 输入两次admin</p><p> <img src="https://img-blog.csdnimg.cn/20190509173009273.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br> <img src="https://img-blog.csdnimg.cn/20190509173024816.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>4.1.12 继续<br> <img src="https://img-blog.csdnimg.cn/20190509173030830.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>4.1.13 等待服务安装和启动<br> <img src="https://img-blog.csdnimg.cn/20190509173035505.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>4.2 安装Hive<br>待续。。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;第1章-Ambari简述&quot;&gt;&lt;a href=&quot;#第1章-Ambari简述&quot; class=&quot;headerlink&quot; title=&quot;第1章 Ambari简述&quot;&gt;&lt;/a&gt;第1章 Ambari简述&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;1.1 什么是Ambari&lt;/strong&gt;&lt;br&gt;Apache Ambari项目旨在通过开发用于配置，管理和监控Apache Hadoop集群的软件来简化Hadoop管理。Ambari提供了一个直观，易用的Hadoop管理Web UI。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Ambari" scheme="https://weiye301.github.io/categories/Ambari/"/>
    
    
      <category term="Ambari" scheme="https://weiye301.github.io/tags/Ambari/"/>
    
      <category term="BigData" scheme="https://weiye301.github.io/tags/BigData/"/>
    
  </entry>
  
  <entry>
    <title>clickhouse的安装和使用（单机+集群）</title>
    <link href="https://weiye301.github.io/2019/05/08/clickhouse%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8%EF%BC%88%E5%8D%95%E6%9C%BA+%E9%9B%86%E7%BE%A4%EF%BC%89/"/>
    <id>https://weiye301.github.io/2019/05/08/clickhouse%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8%EF%BC%88%E5%8D%95%E6%9C%BA+%E9%9B%86%E7%BE%A4%EF%BC%89/</id>
    <published>2019-05-08T03:51:54.000Z</published>
    <updated>2019-11-20T02:40:15.180Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是clickhous"><a href="#什么是clickhous" class="headerlink" title="什么是clickhous"></a>什么是clickhous</h2><pre><code> ClickHouse 是俄罗斯的Yandex于2016年开源的列式存储数据库（DBMS），主要用于在线分析处理查询（OLAP），能够使用SQL查询实时生成分析数据报告。</code></pre><a id="more"></a><h2 id="安装前的准备"><a href="#安装前的准备" class="headerlink" title="安装前的准备"></a>安装前的准备</h2><p>以CentOS6.8为例</p><p> <strong>1. <strong>CentOS取消打开文件数限制</strong></strong><br>    在/etc/security/limits.conf、/etc/security/limits.d/90-nproc.conf这2个文件的末尾加入一下内容：</p><ul><li><p>soft nofile 65536 </p></li><li><p>hard nofile 65536 </p></li><li><p>soft nproc 131072 </p></li><li><p>hard nproc 131072<br>重启生效 用ulimit –n 或者ulimit –a查看设置结果<br>用ulimit –n 或者ulimit –a查看设置结果</p><pre><code>[root@hadoop102 ~]# ulimit -n65536</code></pre></li><li><p><em>2. CentOS取消取消SELINU*</em></p><pre><code>修改/etc/selinux/config中的SELINUX=disabled后重启</code></pre><p>   vim /etc/selinux/config<br>   SELINUX=disabled</p></li><li><p><em>3. CentOS关闭防火墙*</em></p><pre><code>service iptables stop service ip6tables stop</code></pre></li></ul><p><strong>4. 安装依赖</strong></p><pre><code>   yum install -y libtool   yum install -y *unixODBC*</code></pre><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><strong>1.网址</strong></p><p><a href="https://clickhouse.yandex/" target="_blank" rel="noopener">官网</a><br><a href="https://packagecloud.io/altinity/clickhouse" target="_blank" rel="noopener">安装包下载地址</a></p><p><strong>2.单机模式</strong></p><p><strong>上传5个文件到Linux中</strong></p><pre><code>[root@hadoop102 software]# lsclickhouse-client-1.1.54236-4.el6.x86_64.rpm      clickhouse-server-1.1.54236-4.el6.x86_64.rpmclickhouse-compressor-1.1.54236-4.el6.x86_64.rpm  clickhouse-server-common-1.1.54236-4.el6.x86_64.rpmclickhouse-debuginfo-1.1.54236-4.el6.x86_64.rpm</code></pre><p><strong>分别安装这5个rpm文件</strong></p><pre><code>[root@hadoop102 software]#  rpm -ivh *.rpm </code></pre><p><strong>启动ClickServer</strong><br>前台启动：</p><pre><code>clickhouse-server –config-file=/etc/clickhouse-server/config.xml</code></pre><p>后台启动：</p><pre><code>nohup clickhouse-server –config-file=/etc/clickhouse-server/config.xml  &gt;null 2&gt;&amp;1 [1] 2696</code></pre><p><strong>使用client连接server</strong></p><pre><code>clickhouse-client </code></pre><p><strong>3.分布式安装</strong></p><p><strong>准备三台机器，改好主机名之类的，然后执行以上所有步骤</strong><br>我这里是hadoop102,hadoop103,hadoop104</p><p><strong>三台机器修改配置文件config.xml</strong></p><pre><code>vim /etc/clickhouse-server/config.xml把60行左右的三行改为这样    &lt;listen_host&gt;::&lt;/listen_host&gt;    &lt;!-- &lt;listen_host&gt;::1&lt;/listen_host&gt; --&gt;    &lt;!-- &lt;listen_host&gt;127.0.0.1&lt;/listen_host&gt; --&gt;</code></pre><p><strong>在三台机器的etc目录下新建metrika.xml文件</strong></p><pre><code>vim /etc/metrika.xml添加如下内容：&lt;yandex&gt;&lt;clickhouse_remote_servers&gt;    &lt;perftest_3shards_1replicas&gt;        &lt;shard&gt;             &lt;internal_replication&gt;true&lt;/internal_replication&gt;            &lt;replica&gt;                &lt;host&gt;hadoop102&lt;/host&gt;                &lt;port&gt;9000&lt;/port&gt;            &lt;/replica&gt;        &lt;/shard&gt;        &lt;shard&gt;            &lt;replica&gt;                &lt;internal_replication&gt;true&lt;/internal_replication&gt;                &lt;host&gt;hadoop103&lt;/host&gt;                &lt;port&gt;9000&lt;/port&gt;            &lt;/replica&gt;        &lt;/shard&gt;        &lt;shard&gt;            &lt;internal_replication&gt;true&lt;/internal_replication&gt;            &lt;replica&gt;                &lt;host&gt;hadoop104&lt;/host&gt;                &lt;port&gt;9000&lt;/port&gt;            &lt;/replica&gt;        &lt;/shard&gt;    &lt;/perftest_3shards_1replicas&gt;&lt;/clickhouse_remote_servers&gt;&lt;zookeeper-servers&gt;  &lt;node index=&quot;1&quot;&gt;    &lt;host&gt;hadoop102&lt;/host&gt;    &lt;port&gt;2181&lt;/port&gt;  &lt;/node&gt;  &lt;node index=&quot;2&quot;&gt;    &lt;host&gt;hadoop103&lt;/host&gt;    &lt;port&gt;2181&lt;/port&gt;  &lt;/node&gt;  &lt;node index=&quot;3&quot;&gt;    &lt;host&gt;hadoop104&lt;/host&gt;    &lt;port&gt;2181&lt;/port&gt;  &lt;/node&gt;&lt;/zookeeper-servers&gt;&lt;macros&gt;    &lt;replica&gt;hadoop102&lt;/replica&gt;&lt;/macros&gt;&lt;networks&gt;   &lt;ip&gt;::/0&lt;/ip&gt;&lt;/networks&gt;&lt;clickhouse_compression&gt;&lt;case&gt;  &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt;  &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt;                                                                                                                                         &lt;method&gt;lz4&lt;/method&gt;&lt;/case&gt;&lt;/clickhouse_compression&gt;&lt;/yandex&gt;</code></pre><p><strong>注意：</strong>    </p><pre><code>&lt;macros&gt;    &lt;replica&gt;hadoop102&lt;/replica&gt;&lt;/macros&gt;</code></pre><p>不同机器这里不能相同</p><p><strong>三台机器启动ClickServer</strong></p><pre><code>service clickhouse-server start</code></pre><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p> <strong>整型</strong><br>固定长度的整型，包括有符号整型或无符号整型。<br>整型范：<br>Int8 - [-128 : 127]<br>Int16 - [-32768 : 32767]<br>Int32 - [-2147483648 : 2147483647]<br>Int64 - [-9223372036854775808 : 9223372036854775807]<br>无符号整型范：<br>UInt8 - [0 : 255]<br>UInt16 - [0 : 65535]<br>UInt32 - [0 : 4294967295]<br>UInt64 - [0 : 18446744073709551615]</p><p> <strong>浮点型</strong><br>Float32 - float<br>Float64 – double<br>建议尽可能以整数形式存储数据。例如，将固定精度的数字转换为整数值，如时间用毫秒为单位表示，因为浮点型进行计算时可能引起四舍五入的误差。</p><pre><code>select 1-0.9┌───────minus(1, 0.9)─┐│ 0.09999999999999998 │└─────────────────────┘</code></pre><p>与标准SQL相比，ClickHouse 支持以下类别的浮点数：</p><p>Inf-正无穷</p><pre><code>select 1/0┌─divide(1, 0)─┐│          inf │└──────────────┘</code></pre><p>-Inf-负无穷：</p><pre><code>select -1/0┌─divide(1, 0)─┐│          -inf │└──────────────┘</code></pre><p>NaN-非数字：</p><pre><code>:) select 0/0┌─divide(0, 0)─┐│          nan │└──────────────┘</code></pre><p><strong>布尔型</strong><br>没有单独的类型来存储布尔值。可以使用 UInt8 类型，取值限制为 0 或 1。</p><p><strong>字符串</strong><br>String<br>    字符串可以任意长度的。它可以包含任意的字节集，包含空字节。<br>FixedString(N)<br>    固定长度 N 的字符串，N 必须是严格的正自然数。当服务端读取长度小于 N 的字符串时候，通过在字符串末尾添加空字节来达到 N 字节长度。 当服务端读取长度大于 N 的字符串时候，将返回错误消息。<br>    与String相比，极少会使用FixedString，因为使用起来不是很方便。</p><p><strong>枚举类型</strong><br>包括 Enum8 和 Enum16 类型<br>Enum 保存 ‘string’= integer 的对应关系。<br>Enum8 用 ‘String’= Int8 对描述。<br>Enum16 用 ‘String’= Int16 对描述。<br>用法演示：<br>创建一个带有一个枚举 Enum8(‘hello’ = 1, ‘world’ = 2) 类型的列：</p><pre><code>CREATE TABLE t_enum(    x Enum8(&#39;hello&#39; = 1, &#39;world&#39; = 2))ENGINE = TinyLog</code></pre><p>这个 x 列只能存储类型定义中列出的值：’hello’或’world’。如果尝试保存任何其他值，ClickHouse 抛出异常。</p><pre><code>    INSERT INTO t_enum VALUES (&#39;hello&#39;), (&#39;world&#39;), (&#39;hello&#39;)    INSERT INTO t_enum VALUES    Ok.3 rows in set. Elapsed: 0.002 sec.insert into t_enum values(&#39;a&#39;)INSERT INTO t_enum VALUESException on client:Code: 49. DB::Exception: Unknown element &#39;a&#39; for type Enum8(&#39;hello&#39; = 1, &#39;world&#39; = 2)</code></pre><p>从表中查询数据时，ClickHouse 从 Enum 中输出字符串值。</p><pre><code>SELECT * FROM t_enum┌─x─────┐│ hello ││ world ││ hello │└───────┘</code></pre><p>如果需要看到对应行的数值，则必须将 Enum 值转换为整数类型。</p><pre><code>SELECT CAST(x, &#39;Int8&#39;) FROM t_enum┌─CAST(x, &#39;Int8&#39;)─┐│               1 ││               2 ││               1 │└─────────────────┘</code></pre><p><strong>数组</strong><br>Array(T)：由 T 类型元素组成的数组。<br>T 可以是任意类型，包含数组类型。 但不推荐使用多维数组，ClickHouse 对多维数组的支持有限。例如，不能在 MergeTree 表中存储多维数组。<br>可以使用array函数来创建数组：</p><pre><code>array(T)</code></pre><p>也可以使用方括号：</p><pre><code>[]</code></pre><p>创建数组案例：</p><pre><code>SELECT array(1, 2) AS x, toTypeName(x)SELECT    [1, 2] AS x,    toTypeName(x)┌─x─────┬─toTypeName(array(1, 2))─┐│ [1,2] │ Array(UInt8)            │└───────┴─────────────────────────┘1 rows in set. Elapsed: 0.002 sec.:) SELECT [1, 2] AS x, toTypeName(x)SELECT    [1, 2] AS x,    toTypeName(x)┌─x─────┬─toTypeName([1, 2])─┐│ [1,2] │ Array(UInt8)       │└───────┴────────────────────┘1 rows in set. Elapsed: 0.002 sec.</code></pre><p><strong>元组</strong><br>Tuple(T1, T2, …)：元组，其中每个元素都有单独的类型。<br>创建元组的示例：</p><pre><code>:) SELECT tuple(1,&#39;a&#39;) AS x, toTypeName(x)SELECT    (1, &#39;a&#39;) AS x,    toTypeName(x)┌─x───────┬─toTypeName(tuple(1, &#39;a&#39;))─┐│ (1,&#39;a&#39;) │ Tuple(UInt8, String)      │└─────────┴───────────────────────────┘1 rows in set. Elapsed: 0.021 sec.</code></pre><p><strong>Date</strong><br>日期类型，用两个字节存储，表示从 1970-01-01 (无符号) 到当前的日期值。</p><p>还有很多数据结构，可以参考官方文档 ：<a href="https://clickhouse.yandex/docs/zh/data_types/" target="_blank" rel="noopener">官方文档</a></p><h2 id="表引擎"><a href="#表引擎" class="headerlink" title="表引擎"></a>表引擎</h2><p>表引擎（即表的类型）决定了：<br>1）数据的存储方式和位置，写到哪里以及从哪里读取数据<br>2）支持哪些查询以及如何支持。<br>3）并发数据访问。<br>4）索引的使用（如果存在）。<br>5）是否可以执行多线程请求。<br>6）数据复制参数。<br>ClickHouse的表引擎有很多，下面介绍其中几种，对其他引擎有兴趣的可以去查阅官方文档：<a href="https://clickhouse.yandex/docs/zh/operations/table_engines/" target="_blank" rel="noopener">官方文档</a></p><p><strong>TinyLog</strong><br>最简单的表引擎，用于将数据存储在磁盘上。每列都存储在单独的压缩文件中，写入时，数据将附加到文件末尾。<br>该引擎没有并发控制 </p><ul><li>如果同时从表中读取和写入数据，则读取操作将抛出异常；</li><li>如果同时写入多个查询中的表，则数据将被破坏。</li></ul><p>这种表引擎的典型用法是 write-once：首先只写入一次数据，然后根据需要多次读取。此引擎适用于相对较小的表（建议最多1,000,000行）。如果有许多小表，则使用此表引擎是适合的，因为它比需要打开的文件更少。当拥有大量小表时，可能会导致性能低下。      不支持索引。<br>案例：创建一个TinyLog引擎的表并插入一条数据</p><pre><code>:)create table t (a UInt16, b String) ENGINE = TinyLog;:)insert into t (a, b) values (1, &#39;abc&#39;);</code></pre><p>此时我们到保存数据的目录<code>/var/lib/clickhouse/data/default/t</code>中可以看到如下目录结构：</p><pre><code>[root@hadoop102 t]# lsa.bin  b.bin  sizes.json</code></pre><p>a.bin 和 b.bin 是压缩过的对应的列的数据， sizes.json 中记录了每个 *.bin 文件的大小：</p><pre><code>[root@hadoop102 t]# cat sizes.json {&quot;yandex&quot;:{&quot;a%2Ebin&quot;:{&quot;size&quot;:&quot;28&quot;},&quot;b%2Ebin&quot;:{&quot;size&quot;:&quot;30&quot;}}}</code></pre><p><strong>Memory</strong><br>       内存引擎，数据以未压缩的原始形式直接保存在内存当中，服务器重启数据就会消失。读写操作不会相互阻塞，不支持索引。简单查询下有非常非常高的性能表现（超过10G/s）。<br>        一般用到它的地方不多，除了用来测试，就是在需要非常高的性能，同时数据量又不太大（上限大概 1 亿行）的场景。</p><p><strong>Merge</strong><br>Merge 引擎 (不要跟 MergeTree 引擎混淆) 本身不存储数据，但可用于同时从任意多个其他的表中读取数据。 读是自动并行的，不支持写入。读取时，那些被真正读取到数据的表的索引（如果有的话）会被使用。<br>Merge 引擎的参数：一个数据库名和一个用于匹配表名的正则表达式。<br>案例：先建t1，t2，t3三个表，然后用 Merge 引擎的 t 表再把它们链接起来。</p><pre><code>:)create table t1 (id UInt16, name String) ENGINE=TinyLog;:)create table t2 (id UInt16, name String) ENGINE=TinyLog;:)create table t3 (id UInt16, name String) ENGINE=TinyLog;:)insert into t1(id, name) values (1, &#39;first&#39;);:)insert into t2(id, name) values (2, &#39;second&#39;);:)insert into t3(id, name) values (3, &#39;i am in t3&#39;);:)create table t (id UInt16, name String) ENGINE=Merge(currentDatabase(), &#39;^t&#39;);:) select * from t;┌─id─┬─name─┐│  2 │ second │└────┴──────┘┌─id─┬─name──┐│  1 │ first │└────┴───────┘┌─id─┬─name───────┐│ 3     │ i am in t3 │└────┴────────────┘</code></pre><p><strong>MergeTree</strong><br>Clickhouse 中最强大的表引擎当属 MergeTree （合并树）引擎及该系列（*MergeTree）中的其他引擎。<br>MergeTree 引擎系列的基本理念如下。当你有巨量数据要插入到表中，你要高效地一批批写入数据片段，并希望这些数据片段在后台按照一定规则合并。相比在插入时不断修改（重写）数据进存储，这种策略会高效很多。<br>格式：</p><pre><code>`ENGINE [=] MergeTree(date-column [, sampling_expression], (primary, key),` index_granularity)</code></pre><p>参数解读：</p><pre><code>date-column — 类型为 Date 的列名。ClickHouse 会自动依据这个列按月创建分区。分区名格式为 &quot;YYYYMM&quot; 。sampling_expression — 采样表达式。(primary, key) — 主键。类型为Tuple()index_granularity — 索引粒度。即索引中相邻”标记”间的数据行数。设为 8192 可以适用大部分场景。</code></pre><p>案例：</p><pre><code>create table mt_table (date  Date, id UInt8, name String) ENGINE=MergeTree(date, (id, name), 8192);insert into mt_table values (&#39;2019-05-01&#39;, 1, &#39;zhangsan&#39;);insert into mt_table values (&#39;2019-06-01&#39;, 2, &#39;lisi&#39;);insert into mt_table values (&#39;2019-05-03&#39;, 3, &#39;wangwu&#39;);在/var/lib/clickhouse/data/default/mt_tree下可以看到：[root@hadoop102 mt_table]# ls20190501_20190501_2_2_0  20190503_20190503_6_6_0  20190601_20190601_4_4_0  detached</code></pre><p>随便进入一个目录：</p><pre><code>[root@hadoop102 20190601_20190601_4_4_0]# lschecksums.txt  columns.txt  date.bin  date.mrk  id.bin  id.mrk  name.bin  name.mrk  primary.idx</code></pre><ul><li>*.bin是按列保存数据的文件</li><li>*.mrk保存块偏移量</li><li>primary.idx保存主键索引</li></ul><p><strong>ReplacingMergeTree</strong><br>这个引擎是在 MergeTree 的基础上，添加了“处理重复数据”的功能，该引擎和MergeTree的不同之处在于它会删除具有相同主键的重复项。数据的去重只会在合并的过程中出现。合并会在未知的时间在后台进行，所以你无法预先作出计划。有一些数据可能仍未被处理。因此，ReplacingMergeTree 适用于在后台清除重复的数据以节省空间，但是它不保证没有重复的数据出现。<br>格式：</p><pre><code>ENGINE [=] ReplacingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, [ver])</code></pre><p>可以看出他比MergeTree只多了一个ver，这个ver指代版本列。<br>案例：</p><pre><code>create table rmt_table (date  Date, id UInt8, name String,point UInt8) ENGINE= ReplacingMergeTree(date, (id, name), 8192,point);插入一些数据：insert into rmt_table values (&#39;2019-07-10&#39;, 1, &#39;a&#39;, 20);insert into rmt_table values (&#39;2019-07-10&#39;, 1, &#39;a&#39;, 30);insert into rmt_table values (&#39;2019-07-11&#39;, 1, &#39;a&#39;, 20);insert into rmt_table values (&#39;2019-07-11&#39;, 1, &#39;a&#39;, 30);insert into rmt_table values (&#39;2019-07-11&#39;, 1, &#39;a&#39;, 10);等待一段时间或optimize table rmt_table手动触发merge，后查询:) select * from rmt_table;┌───────date─┬─id─┬─name─┬─point─┐│ 2019-07-11 │  1 │ a    │    30 │└────────────┴────┴──────┴───────┘</code></pre><p><strong>SummingMergeTree</strong><br>该引擎继承自 MergeTree。区别在于，当合并 SummingMergeTree 表的数据片段时，ClickHouse 会把所有具有相同主键的行合并为一行，该行包含了被合并的行中具有数值数据类型的列的汇总值。如果主键的组合方式使得单个键值对应于大量的行，则可以显著的减少存储空间并加快数据查询的速度，对于不可加的列，会取一个最先出现的值。<br>语法：</p><pre><code>ENGINE [=] SummingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, [columns])</code></pre><p>参数：</p><pre><code>columns — 包含将要被汇总的列的列名的元组</code></pre><p>案例：</p><pre><code>create table smt_table (date Date, name String, a UInt16, b UInt16) ENGINE=SummingMergeTree(date, (date, name), 8192, (a))</code></pre><p>插入数据：</p><pre><code>insert into smt_table (date, name, a, b) values (&#39;2019-07-10&#39;, &#39;a&#39;, 1, 2);insert into smt_table (date, name, a, b) values (&#39;2019-07-10&#39;, &#39;b&#39;, 2, 1);insert into smt_table (date, name, a, b) values (&#39;2019-07-11&#39;, &#39;b&#39;, 3, 8);insert into smt_table (date, name, a, b) values (&#39;2019-07-11&#39;, &#39;b&#39;, 3, 8);insert into smt_table (date, name, a, b) values (&#39;2019-07-11&#39;, &#39;a&#39;, 3, 1);insert into smt_table (date, name, a, b) values (&#39;2019-07-12&#39;, &#39;c&#39;, 1, 3);</code></pre><p>等待一段时间或optimize table smt_table手动触发merge，后查询</p><pre><code>:) select * from smt_table ┌───────date─┬─name─┬─a─┬─b─┐│ 2019-07-10 │ a    │ 1 │ 2 ││ 2019-07-10 │ b    │ 2 │ 1 ││ 2019-07-11 │ a    │ 3 │ 1 ││ 2019-07-11 │ b    │ 6 │ 8 ││ 2019-07-12 │ c    │ 1 │ 3 │└────────────┴──────┴───┴───┘</code></pre><p>发现2019-07-11，b的a列合并相加了，b列取了8（因为b列为8的数据最先插入）。</p><p> <strong>Distributed</strong><br>分布式引擎，本身不存储数据, 但可以在多个服务器上进行分布式查询。 读是自动并行的。读取时，远程服务器表的索引（如果有的话）会被使用。 </p><pre><code>Distributed(cluster_name, database, table [, sharding_key])</code></pre><p>参数解析：</p><pre><code>cluster_name  - 服务器配置文件中的集群名,在/etc/metrika.xml中配置的database – 数据库名table – 表名sharding_key – 数据分片键</code></pre><p>案例演示：<br>1）在hadoop102，hadoop103，hadoop104上分别创建一个表t</p><pre><code>:)create table t(id UInt16, name String) ENGINE=TinyLog;</code></pre><p>2）在三台机器的t表中插入一些数据</p><pre><code>:)insert into t(id, name) values (1, &#39;zhangsan&#39;);:)insert into t(id, name) values (2, &#39;lisi&#39;);</code></pre><p>3）在hadoop102上创建分布式表</p><pre><code>:)create table dis_table(id UInt16, name String) ENGINE=Distributed(perftest_3shards_1replicas, default, t, id);</code></pre><p>4）往dis_table中插入数据</p><pre><code>:) insert into dis_table select * from t</code></pre><p>5）查看数据量</p><pre><code>:) select count() from dis_table FROM dis_table ┌─count()─┐│       8 │└─────────┘:) select count() from tSELECT count()FROM t ┌─count()─┐│       3 │└─────────┘</code></pre><p>可以看到每个节点大约有1/3的数据</p><h2 id="SQL语法"><a href="#SQL语法" class="headerlink" title="SQL语法"></a>SQL语法</h2><p><strong>CREATE</strong></p><p>CREATE DATABASE<br>用于创建指定名称的数据库，语法如下：</p><pre><code>CREATE DATABASE [IF NOT EXISTS] db_name</code></pre><p>如果查询中存在IF NOT EXISTS，则当数据库已经存在时，该查询不会返回任何错误。</p><pre><code>:) create database test;Ok.0 rows in set. Elapsed: 0.018 sec.</code></pre><p> CREATE TABLE<br>对于创建表，语法如下：</p><pre><code>CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster](    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],    ...) ENGINE = engineDEFAULT expr – 默认值，用法与SQL类似。MATERIALIZED expr – 物化表达式，被该表达式指定的列不能被INSERT，因为它总是被计算出来的。 对于INSERT而言，不需要考虑这些列。 另外，在SELECT查询中如果包含星号，此列不会被查询。ALIAS expr – 别名。</code></pre><p>有三种方式创建表：<br>1）直接创建</p><pre><code>:) create table t1(id UInt16,name String) engine=TinyLog</code></pre><p>2）创建一个与其他表具有相同结构的表</p><pre><code>CREATE TABLE [IF NOT EXISTS] [db.]table_name AS [db2.]name2 [ENGINE = engine]</code></pre><p>可以对其指定不同的表引擎声明。如果没有表引擎声明，则创建的表将与db2.name2使用相同的表引擎。</p><pre><code>:) create table t2 as t1 engine=Memory:) desc t2DESCRIBE TABLE t2┌─name─┬─type───┬─default_type─┬─default_expression─┐│ id   │ UInt16 │              │                    ││ name   │ String │              │                    │└──────┴────────┴──────────────┴────────────────────┘</code></pre><p>3）使用指定的引擎创建一个与SELECT子句的结果具有相同结构的表，并使用SELECT子句的结果填充它。<br>语法：</p><pre><code>CREATE TABLE [IF NOT EXISTS] [db.]table_name ENGINE = engine AS SELECT ...</code></pre><p>实例：<br>先在t2中插入几条数据</p><pre><code>:) insert into t1 values(1,&#39;zhangsan&#39;),(2,&#39;lisi&#39;),(3,&#39;wangwu&#39;):) create table t3 engine=TinyLog as select * from t1:) select * from t3┌─id─┬─name─────┐│  1 │ zhangsan ││  2 │ lisi     ││  3 │ wangwu   │└────┴──────────┘</code></pre><p> <strong>INSERT INTO</strong><br>主要用于向表中添加数据，基本格式如下：</p><pre><code>INSERT INTO [db.]table [(c1, c2, c3)] VALUES (v11, v12, v13), (v21, v22, v23), ...</code></pre><p>实例：</p><pre><code>:) insert into t1 values(1,&#39;zhangsan&#39;),(2,&#39;lisi&#39;),(3,&#39;wangwu&#39;)</code></pre><p>还可以使用select来写入数据：</p><pre><code>INSERT INTO [db.]table [(c1, c2, c3)] SELECT ...    实例：:) insert into t2 select * from t3:) select * from t2┌─id─┬─name─────┐│  1 │ zhangsan ││  2 │ lisi     ││  3 │ wangwu   │└────┴──────────┘</code></pre><p>ClickHouse不支持的修改数据的查询：UPDATE, DELETE, REPLACE, MERGE, UPSERT, INSERT UPDATE。</p><p><strong>ALTER</strong><br>ALTER只支持MergeTree系列，Merge和Distributed引擎的表，基本语法：</p><pre><code>ALTER TABLE [db].name [ON CLUSTER cluster] ADD|DROP|MODIFY COLUMN ...</code></pre><p>参数解析：</p><pre><code>ADD COLUMN – 向表中添加新列DROP COLUMN – 在表中删除列MODIFY COLUMN – 更改列的类型</code></pre><p>案例演示：<br>1）创建一个MergerTree引擎的表</p><pre><code>create table mt_table (date  Date, id UInt8, name String) ENGINE=MergeTree(date, (id, name), 8192);</code></pre><p>2）向表中插入一些值</p><pre><code>insert into mt_table values (&#39;2019-05-01&#39;, 1, &#39;zhangsan&#39;);insert into mt_table values (&#39;2019-06-01&#39;, 2, &#39;lisi&#39;);insert into mt_table values (&#39;2019-05-03&#39;, 3, &#39;wangwu&#39;);</code></pre><p>3）在末尾添加一个新列age</p><pre><code>:)alter table mt_table add column age UInt8:)desc mt_table┌─name─┬─type───┬─default_type─┬─default_expression─┐│ date │ Date   │              │                    ││ id   │ UInt8  │              │                    ││ name │ String │              │                    ││ age  │ UInt8  │              │                    │└──────┴────────┴──────────────┴────────────────────┘:) select * from mt_table┌───────date─┬─id─┬─name─┬─age─┐│ 2019-06-01 │  2 │ lisi │   0 │└────────────┴────┴──────┴─────┘┌───────date─┬─id─┬─name─────┬─age─┐│ 2019-05-01 │  1 │ zhangsan │   0 ││ 2019-05-03 │  3 │ wangwu   │   0 │└────────────┴────┴──────────┴─────┘</code></pre><p>4）更改age列的类型</p><pre><code>:)alter table mt_table modify column age UInt16:)desc mt_table┌─name─┬─type───┬─default_type─┬─default_expression─┐│ date │ Date   │              │                    ││ id   │ UInt8  │              │                    ││ name │ String │              │                    ││ age  │ UInt16 │              │                    │└──────┴────────┴──────────────┴────────────────────┘</code></pre><p>5）删除刚才创建的age列</p><pre><code>:)alter table mt_table drop column age:)desc mt_table┌─name─┬─type───┬─default_type─┬─default_expression─┐│ date │ Date   │              │                    ││ id   │ UInt8  │              │                    ││ name │ String │              │                    │└──────┴────────┴──────────────┴────────────────────┘</code></pre><p><strong>DESCRIBE TABLE</strong><br>查看表结构</p><pre><code>:)desc mt_table┌─name─┬─type───┬─default_type─┬─default_expression─┐│ date │ Date   │              │                    ││ id   │ UInt8  │              │                    ││ name │ String │              │                    │└──────┴────────┴──────────────┴────────────────────┘</code></pre><p> <strong>CHECK TABLE</strong><br>检查表中的数据是否损坏，他会返回两种结果：<br>0 – 数据已损坏<br>1 – 数据完整<br>该命令只支持Log，TinyLog和StripeLog引擎。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;什么是clickhous&quot;&gt;&lt;a href=&quot;#什么是clickhous&quot; class=&quot;headerlink&quot; title=&quot;什么是clickhous&quot;&gt;&lt;/a&gt;什么是clickhous&lt;/h2&gt;&lt;pre&gt;&lt;code&gt; ClickHouse 是俄罗斯的Yandex于2016年开源的列式存储数据库（DBMS），主要用于在线分析处理查询（OLAP），能够使用SQL查询实时生成分析数据报告。&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
    
      <category term="数据库" scheme="https://weiye301.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="clickhouse" scheme="https://weiye301.github.io/tags/clickhouse/"/>
    
  </entry>
  
  <entry>
    <title>命令行执行可以，脚本无效的解决办法</title>
    <link href="https://weiye301.github.io/2019/02/05/%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%89%A7%E8%A1%8C%E5%8F%AF%E4%BB%A5%EF%BC%8C%E8%84%9A%E6%9C%AC%E6%97%A0%E6%95%88%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/"/>
    <id>https://weiye301.github.io/2019/02/05/%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%89%A7%E8%A1%8C%E5%8F%AF%E4%BB%A5%EF%BC%8C%E8%84%9A%E6%9C%AC%E6%97%A0%E6%95%88%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/</id>
    <published>2019-02-05T05:12:11.000Z</published>
    <updated>2019-11-20T02:40:50.180Z</updated>
    
    <content type="html"><![CDATA[<ul><li>今天在CDH上模拟数据的时候发现，使用通过命令行的方式调用java代码模拟数据可以成功运行，但是放到脚本中就失效。</li></ul><p><strong>我是这么解决的：</strong></p><a id="more"></a><ul><li><input disabled="" type="checkbox"> <p>原来的脚本：</p><p> #! /bin/bash</p><pre><code> for i in hadoop102 hadoop103  do     ssh $i java -classpath /opt/module/log-collector-1.0-SNAPSHOT-jar-with-dependencies.jar com.wei.appclient.AppMain $1 $2 &gt;/opt/module/test.log &amp;&quot; done</code></pre></li></ul><p>执行完可以生成test.log，但是内容为空。</p><p>解决：</p><pre><code>#! /bin/bash    for i in hadoop102 hadoop103     do        ssh $i &quot;source /etc/profile &amp;&amp; java -classpath /opt/module/log-collector-1.0-SNAPSHOT-jar-with-dependencies.jar com.wei.appclient.AppMain $1 $2 &gt;/opt/module/test.log &amp;&quot;    done</code></pre><p>加了个source让ssh后刷新环境变量。</p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;今天在CDH上模拟数据的时候发现，使用通过命令行的方式调用java代码模拟数据可以成功运行，但是放到脚本中就失效。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;我是这么解决的：&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="问题" scheme="https://weiye301.github.io/categories/%E9%97%AE%E9%A2%98/"/>
    
    
      <category term="Linux" scheme="https://weiye301.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Save Modes（保存模式）</title>
    <link href="https://weiye301.github.io/2018/12/27/Save-Modes%EF%BC%88%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%BC%8F%EF%BC%89/"/>
    <id>https://weiye301.github.io/2018/12/27/Save-Modes%EF%BC%88%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%BC%8F%EF%BC%89/</id>
    <published>2018-12-27T06:50:38.000Z</published>
    <updated>2019-12-27T06:51:29.909Z</updated>
    
    <content type="html"><![CDATA[<p>Save Modes（保存模式）<br>Save operations（保存操作）可以选择使用 SaveMode，它指定如何处理现有数据如果存在的话。重要的是要意识到，这些 save modes（保存模式）不使用任何 locking（锁定）并且不是 atomic（原子）。另外，当执行 Overwrite 时，数据将在新数据写出之前被删除。</p><a id="more"></a><p>Scala/Java    Any Language    Meaning<br>SaveMode.ErrorIfExists (default)    “error” (default)    将 DataFrame 保存到 data source（数据源）时，如果数据已经存在，则会抛出异常。<br>SaveMode.Append    “append”    将 DataFrame 保存到 data source（数据源）时，如果 data/table 已存在，则 DataFrame 的内容将被 append（附加）到现有数据中。<br>SaveMode.Overwrite    “overwrite”    Overwrite mode（覆盖模式）意味着将 DataFrame 保存到 data source（数据源）时，如果 data/table 已经存在，则预期 DataFrame 的内容将 overwritten（覆盖）现有数据。<br>SaveMode.Ignore    “ignore”    Ignore mode（忽略模式）意味着当将 DataFrame 保存到 data source（数据源）时，如果数据已经存在，则保存操作预期不会保存 DataFrame 的内容，并且不更改现有数据。这与 SQL 中的 CREATE TABLE IF NOT EXISTS 类似。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Save Modes（保存模式）&lt;br&gt;Save operations（保存操作）可以选择使用 SaveMode，它指定如何处理现有数据如果存在的话。重要的是要意识到，这些 save modes（保存模式）不使用任何 locking（锁定）并且不是 atomic（原子）。另外，当执行 Overwrite 时，数据将在新数据写出之前被删除。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="https://weiye301.github.io/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://weiye301.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>spark的算子</title>
    <link href="https://weiye301.github.io/2018/12/27/spark%E7%9A%84%E7%AE%97%E5%AD%90/"/>
    <id>https://weiye301.github.io/2018/12/27/spark%E7%9A%84%E7%AE%97%E5%AD%90/</id>
    <published>2018-12-27T03:48:19.000Z</published>
    <updated>2019-12-27T06:51:25.079Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Transformations（转换）"><a href="#Transformations（转换）" class="headerlink" title="Transformations（转换）"></a><a href="http://spark.apachecn.org/#/docs/4?id=transformations（转换）" target="_blank" rel="noopener">Transformations（转换）</a></h3><p>下表列出了一些 Spark 常用的 transformations（转换）。详情请参考 RDD API 文档（<a href="http://spark.apachecn.org/#/api/scala/index.html?id=org.apache.spark.rdd.rdd" target="_blank" rel="noopener">Scala</a>，<a href="http://spark.apachecn.org/#/api/java/index.html?org%2Fapache%2Fspark%2Fapi%2Fjava%2FJavaRDD.html" target="_blank" rel="noopener">Java</a>，<a href="http://spark.apachecn.org/#/api/python/pyspark.html?id=pyspark.rdd" target="_blank" rel="noopener">Python</a>，<a href="http://spark.apachecn.org/#/api/R/index.html" target="_blank" rel="noopener">R</a>）和 pair RDD 函数文档（<a href="http://spark.apachecn.org/#/api/scala/index.html?id=org.apache.spark.rdd.pairrddfunctions" target="_blank" rel="noopener">Scala</a>，<a href="http://spark.apachecn.org/#/api/java/index.html?org%2Fapache%2Fspark%2Fapi%2Fjava%2FJavaPairRDD.html" target="_blank" rel="noopener">Java</a>）。</p><a id="more"></a><table><thead><tr><th>Transformation（转换）</th><th>Meaning（含义）</th></tr></thead><tbody><tr><td><strong>map</strong>(<em>func</em>)</td><td>返回一个新的 distributed dataset（分布式数据集），它由每个 source（数据源）中的元素应用一个函数 <em>func</em> 来生成。</td></tr><tr><td><strong>filter</strong>(<em>func</em>)</td><td>返回一个新的 distributed dataset（分布式数据集），它由每个 source（数据源）中应用一个函数 <em>func</em> 且返回值为 true 的元素来生成。</td></tr><tr><td><strong>flatMap</strong>(<em>func</em>)</td><td>与 map 类似，但是每一个输入的 item 可以被映射成 0 个或多个输出的 items（所以 <em>func</em> 应该返回一个 Seq 而不是一个单独的 item）。</td></tr><tr><td><strong>mapPartitions</strong>(<em>func</em>)</td><td>与 map 类似，但是单独的运行在在每个 RDD 的 partition（分区，block）上，所以在一个类型为 T 的 RDD 上运行时 <em>func</em> 必须是 Iterator<T> =&gt; Iterator<U> 类型。</td></tr><tr><td><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td><td>与 mapPartitions 类似，但是也需要提供一个代表 partition 的 index（索引）的 interger value（整型值）作为参数的 <em>func_，所以在一个类型为 T 的 RDD 上运行时 _func</em> 必须是 (Int, Iterator<T>) =&gt; Iterator<U> 类型。</td></tr><tr><td><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td><td>样本数据，设置是否放回（withReplacement），采样的百分比（_fraction_）、使用指定的随机数生成器的种子（seed）。</td></tr><tr><td><strong>union</strong>(<em>otherDataset</em>)</td><td>反回一个新的 dataset，它包含了 source dataset（源数据集）和 otherDataset（其它数据集）的并集。</td></tr><tr><td><strong>intersection</strong>(<em>otherDataset</em>)</td><td>返回一个新的 RDD，它包含了 source dataset（源数据集）和 otherDataset（其它数据集）的交集。</td></tr><tr><td><strong>distinct</strong>([_numTasks_]))</td><td>返回一个新的 dataset，它包含了 source dataset（源数据集）中去重的元素。</td></tr><tr><td><strong>groupByKey</strong>([_numTasks_])</td><td>在一个 (K, V) pair 的 dataset 上调用时，返回一个 (K, Iterable<V>) .</td></tr><tr><td><strong>Note:</strong> 如果分组是为了在每一个 key 上执行聚合操作（例如，sum 或 average)，此时使用 <code>reduceByKey</code> 或 <code>aggregateByKey</code> 来计算性能会更好.</td><td></td></tr><tr><td><strong>Note:</strong> 默认情况下，并行度取决于父 RDD 的分区数。可以传递一个可选的 <code>numTasks</code> 参数来设置不同的任务数。</td><td></td></tr><tr><td><strong>reduceByKey</strong>(<em>func</em>, [_numTasks_])</td><td>在 (K, V) pairs 的 dataset 上调用时，返回 dataset of (K, V) pairs 的 dataset，其中的 values 是针对每个 key 使用给定的函数 <em>func</em> 来进行聚合的，它必须是 type (V,V) =&gt; V 的类型。像 <code>groupByKey</code> 一样，reduce tasks 的数量是可以通过第二个可选的参数来配置的。</td></tr><tr><td><strong>aggregateByKey</strong>(<em>zeroValue</em>)(<em>seqOp</em>, <em>combOp</em>, [_numTasks_])</td><td>在 (K, V) pairs 的 dataset 上调用时，返回 (K, U) pairs 的 dataset，其中的 values 是针对每个 key 使用给定的 combine 函数以及一个 neutral “0” 值来进行聚合的。允许聚合值的类型与输入值的类型不一样，同时避免不必要的配置。像 <code>groupByKey</code> 一样，reduce tasks 的数量是可以通过第二个可选的参数来配置的。</td></tr><tr><td><strong>sortByKey</strong>([_ascending_], [_numTasks_])</td><td>在一个 (K, V) pair 的 dataset 上调用时，其中的 K 实现了 Ordered，返回一个按 keys 升序或降序的 (K, V) pairs 的 dataset，由 boolean 类型的 <code>ascending</code> 参数来指定。</td></tr><tr><td><strong>join</strong>(<em>otherDataset</em>, [_numTasks_])</td><td>在一个 (K, V) 和 (K, W) 类型的 dataset 上调用时，返回一个 (K, (V, W)) pairs 的 dataset，它拥有每个 key 中所有的元素对。Outer joins 可以通过 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 来实现。</td></tr><tr><td><strong>cogroup</strong>(<em>otherDataset</em>, [_numTasks_])</td><td>在一个 (K, V) 和的 dataset 上调用时，返回一个 (K, (Iterable<V>, Iterable<W>)) tuples 的 dataset。这个操作也调用了 <code>groupWith</code>。</td></tr><tr><td><strong>cartesian</strong>(<em>otherDataset</em>)</td><td>在一个 T 和 U 类型的 dataset 上调用时，返回一个 (T, U) pairs 类型的 dataset（所有元素的 pairs，即笛卡尔积）。</td></tr><tr><td><strong>pipe</strong>(<em>command</em>, <em>[envVars]</em>)</td><td>通过使用 shell 命令来将每个 RDD 的分区给 Pipe。例如，一个 Perl 或 bash 脚本。RDD 的元素会被写入进程的标准输入（stdin），并且 lines（行）输出到它的标准输出（stdout）被作为一个字符串型 RDD 的 string 返回。</td></tr><tr><td><strong>coalesce</strong>(<em>numPartitions</em>)</td><td>Decrease（降低）RDD 中 partitions（分区）的数量为 numPartitions。对于执行过滤后一个大的 dataset 操作是更有效的。</td></tr><tr><td><strong>repartition</strong>(<em>numPartitions</em>)</td><td>Reshuffle（重新洗牌）RDD 中的数据以创建或者更多的 partitions（分区）并将每个分区中的数据尽量保持均匀。该操作总是通过网络来 shuffles 所有的数据。</td></tr><tr><td><strong>repartitionAndSortWithinPartitions</strong>(<em>partitioner</em>)</td><td>根据给定的 partitioner（分区器）对 RDD 进行重新分区，并在每个结果分区中，按照 key 值对记录排序。这比每一个分区中先调用 <code>repartition</code> 然后再 sorting（排序）效率更高，因为它可以将排序过程推送到 shuffle 操作的机器上进行。</td></tr></tbody></table><h3 id="Actions（动作）"><a href="#Actions（动作）" class="headerlink" title="Actions（动作）"></a><a href="http://spark.apachecn.org/#/docs/4?id=actions（动作）" target="_blank" rel="noopener">Actions（动作）</a></h3><p>下表列出了一些 Spark 常用的 actions 操作。详细请参考 RDD API 文档（<a href="http://spark.apachecn.org/#/api/scala/index.html?id=org.apache.spark.rdd.rdd" target="_blank" rel="noopener">Scala</a>，<a href="http://spark.apachecn.org/#/api/java/index.html?org%2Fapache%2Fspark%2Fapi%2Fjava%2FJavaRDD.html" target="_blank" rel="noopener">Java</a>，<a href="http://spark.apachecn.org/#/api/python/pyspark.html?id=pyspark.rdd" target="_blank" rel="noopener">Python</a>，<a href="http://spark.apachecn.org/#/api/R/index.html" target="_blank" rel="noopener">R</a>）</p><p>和 pair RDD 函数文档（<a href="http://spark.apachecn.org/#/api/scala/index.html?id=org.apache.spark.rdd.pairrddfunctions" target="_blank" rel="noopener">Scala</a>，<a href="http://spark.apachecn.org/#/api/java/index.html?org%2Fapache%2Fspark%2Fapi%2Fjava%2FJavaPairRDD.html" target="_blank" rel="noopener">Java</a>）。</p><table><thead><tr><th>Action（动作）</th><th>Meaning（含义）</th></tr></thead><tbody><tr><td><strong>reduce</strong>(<em>func</em>)</td><td>使用函数 <em>func</em> 聚合 dataset 中的元素，这个函数 <em>func</em> 输入为两个元素，返回为一个元素。这个函数应该是可交换（commutative）和关联（associative）的，这样才能保证它可以被并行地正确计算。</td></tr><tr><td><strong>collect</strong>()</td><td>在 driver 程序中，以一个 array 数组的形式返回 dataset 的所有元素。这在过滤器（filter）或其他操作（other operation）之后返回足够小（sufficiently small）的数据子集通常是有用的。</td></tr><tr><td><strong>count</strong>()</td><td>返回 dataset 中元素的个数。</td></tr><tr><td><strong>first</strong>()</td><td>返回 dataset 中的第一个元素（类似于 take(1)。</td></tr><tr><td><strong>take</strong>(<em>n</em>)</td><td>将数据集中的前 <em>n</em> 个元素作为一个 array 数组返回。</td></tr><tr><td><strong>takeSample</strong>(<em>withReplacement</em>, <em>num</em>, [_seed_])</td><td>对一个 dataset 进行随机抽样，返回一个包含 <em>num</em> 个随机抽样（random sample）元素的数组，参数 withReplacement 指定是否有放回抽样，参数 seed 指定生成随机数的种子。</td></tr><tr><td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td><td>返回 RDD 按自然顺序（natural order）或自定义比较器（custom comparator）排序后的前 <em>n</em> 个元素。</td></tr><tr><td><strong>saveAsTextFile</strong>(<em>path</em>)</td><td>将 dataset 中的元素以文本文件（或文本文件集合）的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中的给定目录中。Spark 将对每个元素调用 toString 方法，将数据元素转换为文本文件中的一行记录。</td></tr><tr><td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td><td></td></tr><tr><td>(Java and Scala)</td><td>将 dataset 中的元素以 Hadoop SequenceFile 的形式写入到本地文件系统、HDFS 或其它 Hadoop 支持的文件系统指定的路径中。该操作可以在实现了 Hadoop 的 Writable 接口的键值对（key-value pairs）的 RDD 上使用。在 Scala 中，它还可以隐式转换为 Writable 的类型（Spark 包括了基本类型的转换，例如 Int，Double，String 等等)。</td></tr><tr><td><strong>saveAsObjectFile</strong>(<em>path</em>)</td><td></td></tr><tr><td>(Java and Scala)</td><td>使用 Java 序列化（serialization）以简单的格式（simple format）编写数据集的元素，然后使用 <code>SparkContext.objectFile()</code> 进行加载。</td></tr><tr><td><strong>countByKey</strong>()</td><td>仅适用于（K,V）类型的 RDD。返回具有每个 key 的计数的（K , Int）pairs 的 hashmap。</td></tr><tr><td><strong>foreach</strong>(<em>func</em>)</td><td>对 dataset 中每个元素运行函数 _func_。这通常用于副作用（side effects），例如更新一个 <a href="http://spark.apachecn.org/#/docs/4?id=accumulators" target="_blank" rel="noopener">Accumulator</a>（累加器）或与外部存储系统（external storage systems）进行交互。<strong>Note</strong>：修改除 <code>foreach()</code>之外的累加器以外的变量（variables）可能会导致未定义的行为（undefined behavior）。详细介绍请阅读 <a href="http://spark.apachecn.org/#/docs/4?id=understanding-closures-a-nameclosureslinka" target="_blank" rel="noopener">Understanding closures（理解闭包）</a> 部分。</td></tr></tbody></table><p>该 Spark RDD API 还暴露了一些 actions（操作）的异步版本，例如针对 <code>foreach</code> 的 <code>foreachAsync</code>，它们会立即返回一个<code>FutureAction</code> 到调用者，而不是在完成 action 时阻塞。这可以用于管理或等待 action 的异步执行。.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Transformations（转换）&quot;&gt;&lt;a href=&quot;#Transformations（转换）&quot; class=&quot;headerlink&quot; title=&quot;Transformations（转换）&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://spark.apachecn.org/#/docs/4?id=transformations（转换）&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Transformations（转换）&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;下表列出了一些 Spark 常用的 transformations（转换）。详情请参考 RDD API 文档（&lt;a href=&quot;http://spark.apachecn.org/#/api/scala/index.html?id=org.apache.spark.rdd.rdd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Scala&lt;/a&gt;，&lt;a href=&quot;http://spark.apachecn.org/#/api/java/index.html?org%2Fapache%2Fspark%2Fapi%2Fjava%2FJavaRDD.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Java&lt;/a&gt;，&lt;a href=&quot;http://spark.apachecn.org/#/api/python/pyspark.html?id=pyspark.rdd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Python&lt;/a&gt;，&lt;a href=&quot;http://spark.apachecn.org/#/api/R/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;R&lt;/a&gt;）和 pair RDD 函数文档（&lt;a href=&quot;http://spark.apachecn.org/#/api/scala/index.html?id=org.apache.spark.rdd.pairrddfunctions&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Scala&lt;/a&gt;，&lt;a href=&quot;http://spark.apachecn.org/#/api/java/index.html?org%2Fapache%2Fspark%2Fapi%2Fjava%2FJavaPairRDD.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Java&lt;/a&gt;）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="https://weiye301.github.io/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://weiye301.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>存储级别</title>
    <link href="https://weiye301.github.io/2018/12/27/%E5%AD%98%E5%82%A8%E7%BA%A7%E5%88%AB/"/>
    <id>https://weiye301.github.io/2018/12/27/%E5%AD%98%E5%82%A8%E7%BA%A7%E5%88%AB/</id>
    <published>2018-12-27T03:47:13.000Z</published>
    <updated>2019-12-27T06:51:16.929Z</updated>
    
    <content type="html"><![CDATA[<p> Spark 的缓存具有容错机制，如果一个缓存的 RDD 的某个分区丢失了，Spark 将按照原来的计算过程，自动重新计算并进行缓存。 </p><a id="more"></a><table><thead><tr><th>Storage Level（存储级别）</th><th>Meaning（含义）</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>将 RDD 以反序列化的 Java 对象的形式存储在 JVM 中。如果内存空间不够，部分数据分区将不再缓存，在每次需要用到这些数据时重新进行计算。这是默认的级别。</td></tr><tr><td>MEMORY_AND_DISK</td><td>将 RDD 以反序列化的 Java 对象的形式存储在 JVM 中。如果内存空间不够，将未缓存的数据分区存储到磁盘，在需要使用这些分区时从磁盘读取。</td></tr><tr><td>MEMORY_ONLY_SER</td><td></td></tr><tr><td>(Java and Scala)</td><td>将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式会比反序列化对象的方式节省很多空间，尤其是在使用 <a href="http://spark.apachecn.org/#/tuning.html" target="_blank" rel="noopener">fast serializer</a> 时会节省更多的空间，但是在读取时会增加 CPU 的计算负担。</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td></td></tr><tr><td>(Java and Scala)</td><td>类似于 MEMORY_ONLY_SER，但是溢出的分区会存储到磁盘，而不是在用到它们时重新计算。</td></tr><tr><td>DISK_ONLY</td><td>只在磁盘上缓存 RDD。</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc。 与上面的级别功能相同，只不过每个分区在集群中两个节点上建立副本。</td><td></td></tr><tr><td>OFF_HEAP（experimental 实验性）</td><td>类似于 MEMORY_ONLY_SER，但是将数据存储在 <a href="http://spark.apachecn.org/#/configuration.html?id=memory-management" target="_blank" rel="noopener">off-heap memory</a> 中。这需要启用 off-heap 内存。</td></tr></tbody></table><p><strong>Note:</strong> <em>在 Python 中，stored objects will 总是使用 <a href="https://docs.python.org/2/library/pickle.html" target="_blank" rel="noopener">Pickle</a> library 来序列化对象，所以无论你选择序列化级别都没关系。在 Python 中可用的存储级别有 <code>MEMORY_ONLY</code>，<code>MEMORY_ONLY_2</code>，<code>MEMORY_AND_DISK</code>，<code>MEMORY_AND_DISK_2</code>，<code>DISK_ONLY</code>，和 <code>DISK_ONLY_2</code>。</em></p><p>在 shuffle 操作中（例如 <code>reduceByKey</code>），即便是用户没有调用 <code>persist</code> 方法，Spark 也会自动缓存部分中间数据.这么做的目的是，在 shuffle 的过程中某个节点运行失败时，不需要重新计算所有的输入数据。如果用户想多次使用某个 RDD，强烈推荐在该 RDD 上调用 persist 方法.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt; Spark 的缓存具有容错机制，如果一个缓存的 RDD 的某个分区丢失了，Spark 将按照原来的计算过程，自动重新计算并进行缓存。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="https://weiye301.github.io/categories/Spark/"/>
    
    
      <category term="Spark" scheme="https://weiye301.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode第203题：移除链表元素</title>
    <link href="https://weiye301.github.io/2018/07/13/LeetCode%E7%AC%AC203%E9%A2%98%EF%BC%9A%E7%A7%BB%E9%99%A4%E9%93%BE%E8%A1%A8%E5%85%83%E7%B4%A0/"/>
    <id>https://weiye301.github.io/2018/07/13/LeetCode%E7%AC%AC203%E9%A2%98%EF%BC%9A%E7%A7%BB%E9%99%A4%E9%93%BE%E8%A1%A8%E5%85%83%E7%B4%A0/</id>
    <published>2018-07-13T02:59:14.000Z</published>
    <updated>2020-01-13T03:02:42.065Z</updated>
    
    <content type="html"><![CDATA[<p>删除链表中等于给定值 val 的所有节点。</p><p>示例:</p><p>输入: <code>1-&gt;2-&gt;6-&gt;3-&gt;4-&gt;5-&gt;6, val = 6</code><br>输出: <code>1-&gt;2-&gt;3-&gt;4-&gt;5</code></p><a id="more"></a><pre><code>public class Solution {    public ListNode removeElements(ListNode head, int val) {        ListNode dummyHead = new ListNode(-1);        dummyHead.next = head;        ListNode prev = dummyHead;        while (prev.next != null) {            if (prev.next.val == val) {                prev.next = prev.next.next;            } else {                prev = prev.next;            }        }        return dummyHead.next;    }}</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;删除链表中等于给定值 val 的所有节点。&lt;/p&gt;
&lt;p&gt;示例:&lt;/p&gt;
&lt;p&gt;输入: &lt;code&gt;1-&amp;gt;2-&amp;gt;6-&amp;gt;3-&amp;gt;4-&amp;gt;5-&amp;gt;6, val = 6&lt;/code&gt;&lt;br&gt;输出: &lt;code&gt;1-&amp;gt;2-&amp;gt;3-&amp;gt;4-&amp;gt;5&lt;/code&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://weiye301.github.io/categories/LeetCode/"/>
    
    
      <category term="LinkedList" scheme="https://weiye301.github.io/tags/LinkedList/"/>
    
      <category term="LeetCode" scheme="https://weiye301.github.io/tags/LeetCode/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode第20题：有效的括号</title>
    <link href="https://weiye301.github.io/2018/07/08/LeetCode%E7%AC%AC20%E9%A2%98%EF%BC%9A%E6%9C%89%E6%95%88%E7%9A%84%E6%8B%AC%E5%8F%B7/"/>
    <id>https://weiye301.github.io/2018/07/08/LeetCode%E7%AC%AC20%E9%A2%98%EF%BC%9A%E6%9C%89%E6%95%88%E7%9A%84%E6%8B%AC%E5%8F%B7/</id>
    <published>2018-07-08T07:14:22.000Z</published>
    <updated>2020-01-13T03:02:20.406Z</updated>
    
    <content type="html"><![CDATA[<p>给定一个只包括 <code>&#39;(&#39;，&#39;)&#39;，&#39;{&#39;，&#39;}&#39;，&#39;[&#39;，&#39;]&#39;</code>的字符串，判断字符串是否有效。</p><p>有效字符串需满足：</p><p>左括号必须用相同类型的右括号闭合。<br>左括号必须以正确的顺序闭合。<br>注意空字符串可被认为是有效字符串。</p><p>示例 1:</p><pre><code>输入: &quot;()&quot;输出: true</code></pre><p>示例 2:</p><pre><code>输入: &quot;()[]{}&quot;输出: true</code></pre><p>示例 3:</p><pre><code>输入: &quot;(]&quot;输出: false</code></pre><p>示例 4:</p><pre><code>输入: &quot;([)]&quot;输出: false</code></pre><p>示例 5:</p><pre><code>输入: &quot;{[]}&quot;输出: true</code></pre><p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode-cn.com/problems/valid-parentheses" target="_blank" rel="noopener">https://leetcode-cn.com/problems/valid-parentheses</a></p><a id="more"></a><pre><code>import java.util.Stack;class Solution {    public boolean isValid(String s) {        Stack&lt;Character&gt; stack = new Stack&lt;&gt;();        for (int i = 0; i &lt; s.length(); i++) {            char c = s.charAt(i);            if (c == &#39;(&#39; || c == &#39;{&#39; || c == &#39;[&#39;) {                stack.push(c);            } else {                if(stack.isEmpty()){                    return false;                }                Character peek = stack.pop();                if (c == &#39;)&#39; &amp;&amp; peek != &#39;(&#39;) return false;                if (c == &#39;]&#39; &amp;&amp; peek != &#39;[&#39;) return false;                if (c == &#39;}&#39; &amp;&amp; peek != &#39;{&#39;) return false;            }        }        return stack.isEmpty();    }}</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;给定一个只包括 &lt;code&gt;&amp;#39;(&amp;#39;，&amp;#39;)&amp;#39;，&amp;#39;{&amp;#39;，&amp;#39;}&amp;#39;，&amp;#39;[&amp;#39;，&amp;#39;]&amp;#39;&lt;/code&gt;的字符串，判断字符串是否有效。&lt;/p&gt;
&lt;p&gt;有效字符串需满足：&lt;/p&gt;
&lt;p&gt;左括号必须用相同类型的右括号闭合。&lt;br&gt;左括号必须以正确的顺序闭合。&lt;br&gt;注意空字符串可被认为是有效字符串。&lt;/p&gt;
&lt;p&gt;示例 1:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;输入: &amp;quot;()&amp;quot;
输出: true&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;示例 2:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;输入: &amp;quot;()[]{}&amp;quot;
输出: true&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;示例 3:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;输入: &amp;quot;(]&amp;quot;
输出: false&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;示例 4:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;输入: &amp;quot;([)]&amp;quot;
输出: false&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;示例 5:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;输入: &amp;quot;{[]}&amp;quot;
输出: true&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;来源：力扣（LeetCode）&lt;br&gt;链接：&lt;a href=&quot;https://leetcode-cn.com/problems/valid-parentheses&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://leetcode-cn.com/problems/valid-parentheses&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://weiye301.github.io/categories/LeetCode/"/>
    
    
      <category term="LeetCode" scheme="https://weiye301.github.io/tags/LeetCode/"/>
    
      <category term="Stack" scheme="https://weiye301.github.io/tags/Stack/"/>
    
  </entry>
  
  <entry>
    <title>数据结构之二分搜索树</title>
    <link href="https://weiye301.github.io/2018/01/13/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%A0%91/"/>
    <id>https://weiye301.github.io/2018/01/13/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E4%BA%8C%E5%88%86%E6%90%9C%E7%B4%A2%E6%A0%91/</id>
    <published>2018-01-13T08:08:33.000Z</published>
    <updated>2020-01-13T08:11:15.952Z</updated>
    
    <content type="html"><![CDATA[<p>使用java实现了二分搜索树</p><a id="more"></a><pre><code>public class BST&lt;E extends Comparable&lt;E&gt;&gt; {    private class Node {        public E e;        public Node left, right;        public Node(E e) {            this.e = e;            right = null;            left = null;        }    }    private Node root;    private int size;    public BST() {        root = null;        size = 0;    }    public int size() {        return size;    }    public boolean isEmpty() {        return size == 0;    }    //向二分搜索树中插入元素    public void add(E e) {        root = add(root, e);    }    //向以node为根节点的树中添加元素    //返回添加完的根节点    private Node add(Node node, E e) {        if (node == null) {            size++;            return new Node(e);        }        if (e.compareTo(node.e) &lt; 0) {            node.left = add(node.left, e);        } else if (e.compareTo(node.e) &gt; 0) {            node.right = add(node.right, e);        }        return node;    }    //查找二分搜索树中是否包含某个元素    public boolean contains(E e) {        return contains(root, e);    }    //查找以node为根的树中是否包含e    private boolean contains(Node node, E e) {        if (node == null) {            return false;        }        if (e.compareTo(node.e) == 0) {            return true;        } else if (e.compareTo(node.e) &lt; 0) {            return contains(node.left, e);        } else {            return contains(node.right, e);        }    }    //前序遍历二叉搜索树    public void perOrder() {        perOrder(root);    }    //使用递归实现前序遍历    private void perOrder(Node node) {        if (node == null) return;        System.out.println(node.e);        perOrder(node.left);        perOrder(node.right);    }    //中序遍历    public void inOrder() {        inOrder(root);    }    //递归实现中序遍历    private void inOrder(Node node) {        if (node == null) return;        inOrder(node.left);        System.out.println(node.e);        inOrder(node.right);    }    //后续遍历    public void afOrder() {        afOrder(root);    }    //递归实现后续遍历    private void afOrder(Node node) {        if (node == null) return;        inOrder(node.left);        inOrder(node.right);        System.out.println(node.e);    }}</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用java实现了二分搜索树&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据结构" scheme="https://weiye301.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="BST" scheme="https://weiye301.github.io/tags/BST/"/>
    
  </entry>
  
  <entry>
    <title>数据结构之用链表实现队列</title>
    <link href="https://weiye301.github.io/2018/01/10/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E7%94%A8%E9%93%BE%E8%A1%A8%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/"/>
    <id>https://weiye301.github.io/2018/01/10/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E7%94%A8%E9%93%BE%E8%A1%A8%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/</id>
    <published>2018-01-10T09:22:21.000Z</published>
    <updated>2020-01-10T09:24:06.932Z</updated>
    
    <content type="html"><![CDATA[<p>时间复杂度为O(1)</p><a id="more"></a><pre><code>public class LinkedListQueue&lt;E&gt; implements Queue&lt;E&gt; {    /**     * 定义一个节点     */    private class Node {        public E e;        public Node next;        public Node(E e, Node next) {            this.e = e;            this.next = next;        }        public Node(E e) {            this(e, null);        }        public Node() {            this(null, null);        }        @Override        public String toString() {            return e.toString();        }    }    //队列大小    private int size;    //head为队首，tail为队尾，元素从队尾进入，从队首删除，这样做可以达到O(1)    private Node head, tail;    public LinkedListQueue() {        size = 0;        head = null;        tail = null;    }    @Override    public boolean isEmpty() {        return size == 0;    }    /**     * 入队     *     * @param e     */    @Override    public void enqueue(E e) {        //tail为空，说明第一次加入元素        if (tail == null) {            tail = new Node(e);            head = tail;        } else {            tail.next = new Node(e);            tail = tail.next;        }        size++;    }    /**     * 出队     *     * @return     */    @Override    public E dequeue() {        if (head == null) {            throw new IllegalArgumentException(&quot;队列空了！&quot;);        }        Node delNode = head;        head = head.next;        delNode.next = null;        //当删除一个元素后需要判断还有没有剩余元素，没有的话tail需要置为空        if (head == null) tail = null;        size--;        return delNode.e;    }    @Override    public int getSize() {        return size;    }    @Override    public E getFront() {        return null;    }    @Override    public String toString() {        StringBuilder res = new StringBuilder();        res.append(&quot;LinkedListQueue tail:&quot;);        for (Node node = head; node != null; node = node.next) {            res.append(node.toString() + &quot;--&gt;&quot;);        }        res.append(&quot;NULL&quot;);        return res.toString();    }}</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;时间复杂度为O(1)&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据结构" scheme="https://weiye301.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="LinkedList" scheme="https://weiye301.github.io/tags/LinkedList/"/>
    
      <category term="Queue" scheme="https://weiye301.github.io/tags/Queue/"/>
    
  </entry>
  
  <entry>
    <title>数据结构之链表</title>
    <link href="https://weiye301.github.io/2018/01/10/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/"/>
    <id>https://weiye301.github.io/2018/01/10/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/</id>
    <published>2018-01-10T08:53:36.000Z</published>
    <updated>2020-01-13T08:09:38.810Z</updated>
    
    <content type="html"><![CDATA[<p>使用java实现了链表</p><a id="more"></a><pre><code>public class LinkedList&lt;E&gt; {    /**     * 定义一个节点     */    private class Node {        public E e;        public Node next;        public Node(E e, Node next) {            this.e = e;            this.next = next;        }        public Node(E e) {            this(e, null);        }        public Node() {            this(null, null);        }        @Override        public String toString() {            return e.toString();        }    }    //记录链表的节点个数    private int size;    //虚拟头结点    private Node dummyHead;    public LinkedList() {        size = 0;        dummyHead = new Node(null, null);    }    /**     * 判断链表是否为空     *     * @return     */    public boolean isEmpty() {        return size == 0;    }    /**     * 获取节点个数     *     * @return     */    public int getSize() {        return size;    }    /**     * 在链表头添加节点     *     * @param e     */    public void addFirst(E e) {        add(0, e);    }    /**     * 在指定位置添加节点     *     * @param index     * @param e     */    public void add(int index, E e) {        if (index &lt; 0 || index &gt; size)            throw new IllegalArgumentException(&quot;index error&quot;);        Node find = dummyHead;        for (int i = 0; i &lt; index; i++) {            find = find.next;        }//        Node node = new Node(e);//        node.next = find.next;//        find.next = node;        find.next = new Node(e, find.next);        size++;    }    /**     * 在链表尾部添加节点     *     * @param e     */    public void addLast(E e) {        add(size, e);    }    /**     * 判断是否包含e     *     * @param e     * @return     */    public boolean contains(E e) {        Node cur = dummyHead.next;        for (int i = 0; i &lt; size; i++) {            if (e.equals(cur.e)) {                return true;            }            cur = cur.next;        }        return false;    }    /**     * 获取指定位置的元素     *     * @param index     * @return     */    public E getForIndex(int index) {        if (index &lt; 0 || index &gt; size)            throw new IllegalArgumentException(&quot;index error&quot;);        Node cur = dummyHead.next;        for (int i = 0; i &lt; index; i++) {            cur = cur.next;        }        return cur.e;    }    /**     * 获取第一个节点元素     *     * @return     */    public E getFirst() {        return getForIndex(0);    }    /**     * 获取最后一个节点值     *     * @return     */    public E getLast() {        return getForIndex(size - 1);    }    /**     * 删除指定位置节点     *     * @param index     * @return     */    public E remove(int index) {        if (index &lt; 0 || index &gt; size)            throw new IllegalArgumentException(&quot;index error&quot;);        Node prve = dummyHead;        for (int i = 0; i &lt; index; i++) {            prve = prve.next;        }        Node delNode = prve.next;        prve.next = delNode.next;        delNode.next = null;        size--;        return delNode.e;    }    public E removeFirst() {        return remove(0);    }    public E removeLast() {        return remove(size - 1);    }    /**     * 修改指定位置的值     * @param index     * @param e     */    public void set(int index, E e) {        if (index &lt; 0 || index &gt; size)            throw new IllegalArgumentException(&quot;index error&quot;);        Node cur = dummyHead.next;        for (int i = 0; i &lt; index; i++) {            cur = cur.next;        }        cur.e = e;    }    @Override    public String toString() {        StringBuilder res = new StringBuilder();        for (Node node = dummyHead.next; node != null; node = node.next) {            res.append(node.toString() + &quot;--&gt;&quot;);        }        res.append(&quot;NULL&quot;);        return res.toString();    }}</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用java实现了链表&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据结构" scheme="https://weiye301.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="LinkedList" scheme="https://weiye301.github.io/tags/LinkedList/"/>
    
  </entry>
  
  <entry>
    <title>数据结构之循环队列</title>
    <link href="https://weiye301.github.io/2018/01/08/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%BE%AA%E7%8E%AF%E9%98%9F%E5%88%97/"/>
    <id>https://weiye301.github.io/2018/01/08/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%BE%AA%E7%8E%AF%E9%98%9F%E5%88%97/</id>
    <published>2018-01-08T09:01:41.000Z</published>
    <updated>2020-01-13T08:09:00.972Z</updated>
    
    <content type="html"><![CDATA[<p>使用java实现了循环队列</p><a id="more"></a><pre><code>import java.util.FormatFlagsConversionMismatchException;public class LoopQueue&lt;E&gt; implements Queue&lt;E&gt; {    private E[] data;    private int front;    private int tail;    private int size;    public LoopQueue(int capacity) {        data = (E[]) new Object[capacity + 1];        front = 0;        tail = 0;        size = 0;    }    public LoopQueue() {        this(10);    }    @Override    public boolean isEmpty() {        return front == tail;    }    @Override    public void enqueue(E e) {        if ((tail + 1) % data.length == front) {            resize(getCapacity() * 2);        }        data[tail] = e;        tail = (tail + 1) % data.length;        size++;    }    private void resize(int capacity) {        E[] newData = (E[]) new Object[capacity + 1];        for (int i = 0; i &lt; size; i++) {            newData[i] = data[(front + i) % data.length];        }        data = newData;        front = 0;        tail = size;    }    @Override    public E dequeue() {        if (isEmpty()) throw new IllegalArgumentException(&quot;队列以空！&quot;);        E first = data[front];        data[front] = null;        front = (front + 1) % data.length;        size--;        if (size &lt;= data.length / 4) resize(data.length / 2);        return first;    }    @Override    public int getSize() {        return size;    }    @Override    public E getFront() {        if (isEmpty()) throw new IllegalArgumentException(&quot;队列以空！&quot;);        return data[front];    }    public int getCapacity() {        return data.length - 1;    }    @Override    public String toString() {        StringBuilder res = new StringBuilder();        res.append(String.format(&quot;LoopQueue: size = %d , capacity = %d\n&quot;, size, data.length - 1));        res.append(&quot;Loop: front [&quot;);        for (int i = front; i != tail; i = (i + 1) % data.length) {            res.append(data[i]);            if ((i + 1) % data.length != tail) {                res.append(&quot;,&quot;);            }        }        res.append(&quot;] tail&quot;);        return res.toString();    }}</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用java实现了循环队列&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据结构" scheme="https://weiye301.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="Queue" scheme="https://weiye301.github.io/tags/Queue/"/>
    
  </entry>
  
  <entry>
    <title>数据结构之动态数组实现</title>
    <link href="https://weiye301.github.io/2018/01/08/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%8A%A8%E6%80%81%E6%95%B0%E7%BB%84%E5%AE%9E%E7%8E%B0/"/>
    <id>https://weiye301.github.io/2018/01/08/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%8A%A8%E6%80%81%E6%95%B0%E7%BB%84%E5%AE%9E%E7%8E%B0/</id>
    <published>2018-01-08T01:31:16.000Z</published>
    <updated>2020-01-13T08:09:25.185Z</updated>
    
    <content type="html"><![CDATA[<p>使用java实现动态扩容缩容的数据</p><a id="more"></a><pre><code>public class Array&lt;E&gt; {    private E[] data;    private int size;    //构造函数，根据传入的参数指定数组的容量    public Array(int capacity) {        data = (E[]) new Object[capacity];        size = 0;    }    //构造函数，无参数时指定默认容量为10    public Array() {        this(10);    }    //获取数组中的元素个数    public int getSize() {        return size;    }    //获取数组容量大小    public int getCapacity() {        return data.length;    }    //判断数组是否为空    public boolean isEmpty() {        return size == 0;    }    //在所有元素后添加一个元素    public void addLast(E e) {        add(size, e);    }    //在指定位置添加新元素    public void add(int index, E e) {        if (index &lt; 0 || index &gt; size) {            throw new IllegalArgumentException(&quot;索引输入有误！&quot;);        }        if (size == data.length) {            resize(size * 2);        }        for (int i = size - 1; i &gt;= index; i--) {            data[i + 1] = data[i];        }        data[index] = e;        size++;    }    private void resize(int newCapacity) {        E[] newArr = (E[]) new Object[newCapacity];        for (int i = 0; i &lt; size; i++) {            newArr[i] = data[i];        }        data = newArr;    }    //在数组头添加新元素    public void addFirst(E e) {        add(0, e);    }    //获取某个位置的元素    public E get(int index) {        if (index &lt; 0 || index &gt;= size) {            throw new IllegalArgumentException(&quot;索引输入有误！&quot;);        }        return data[index];    }    //修改某个位置的元素    public void set(int index, E e) {        if (index &lt; 0 || index &gt;= size) {            throw new IllegalArgumentException(&quot;索引输入有误！&quot;);        }        data[index] = e;    }    //查找某个元素的索引    public int find(E e) {        for (int i = 0; i &lt; size; i++) {            if (e.equals(data[i])) {                return i;            }        }        return -1;    }    //删除某个位置元素    public E remove(int index) {        if (index &lt; 0 || index &gt;= size) {            throw new IllegalArgumentException(&quot;索引输入有误！&quot;);        }        E res = data[index];        for (int i = index + 1; i &lt; size; i++) {            data[i - 1] = data[i];        }        size--;        data[size] = null;        if (size &lt; data.length / 2) {            resize(data.length / 2);        }        return res;    }    //删除第一个元素    public E removeFirst() {        return remove(0);    }    //删除指定元素    public void removeElement(E e) {        int index = find(e);        if (index != -1) {            remove(index);        }    }    //获取数组的所有元素    @Override    public String toString() {        StringBuilder res = new StringBuilder();        res.append(String.format(&quot;Array: size = %d , capacity = %d\n&quot;, size, data.length));        res.append(&quot;Array:[&quot;);        for (int i = 0; i &lt; size; i++) {            res.append(data[i]);            if (i != size - 1) {                res.append(&quot;,&quot;);            }        }        res.append(&quot;]&quot;);        return res.toString();    }}</code></pre><p>复杂度：O(1),O(n),O(lgn),O(n^2)……</p><p>O：简单来说就是算法的运行时间和输入数据之间的关系</p><pre><code class="java">public static int sum(int[] nums){    int sum=0;     for(int num:nums)        sum+=num;     return sum;</code></pre><p>上面的代码时间复杂度为O(n)，其中n是nums中元素的个数，算法时间与n呈线性关系。</p><p>addLast(e)  O(1)</p><p>addFirst(e) O(n)</p><p>add(index,e)  O(n/2)  = O(n)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用java实现动态扩容缩容的数据&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据结构" scheme="https://weiye301.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="Array" scheme="https://weiye301.github.io/tags/Array/"/>
    
  </entry>
  
</feed>
