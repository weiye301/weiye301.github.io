<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>hexo文章标签分类等设置</title>
    <link href="undefined2019/06/16/hexo%E6%96%87%E7%AB%A0%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E7%AD%89%E8%AE%BE%E7%BD%AE/"/>
    <url>2019/06/16/hexo%E6%96%87%E7%AB%A0%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E7%AD%89%E8%AE%BE%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h2 id="文章基本设置"><a href="#文章基本设置" class="headerlink" title="文章基本设置"></a>文章基本设置</h2><hr><p>title: test<br>date: 2019-05-08 11:51:54<br>tags:<br>    -clickhouse<br>comments: true #是否可评论<br>categories: “数据库” #分类</p><hr>]]></content>
    
    
    <categories>
      
      <category>使用</category>
      
    </categories>
    
    
    <tags>
      
      <tag>-hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Centos6.8安装Ambari</title>
    <link href="undefined2019/05/16/Centos6-8%E5%AE%89%E8%A3%85Ambari/"/>
    <url>2019/05/16/Centos6-8%E5%AE%89%E8%A3%85Ambari/</url>
    
    <content type="html"><![CDATA[<h2 id="第1章-Ambari简述"><a href="#第1章-Ambari简述" class="headerlink" title="第1章 Ambari简述"></a>第1章 Ambari简述</h2><p><strong>1.1 什么是Ambari</strong><br>Apache Ambari项目旨在通过开发用于配置，管理和监控Apache Hadoop集群的软件来简化Hadoop管理。Ambari提供了一个直观，易用的Hadoop管理Web UI。<br><strong>1.2 Ambari的功能</strong><br>Ambari提供了跨任意数量的主机安装Hadoop服务的分步向导。<br>Ambari处理群集的Hadoop服务配置。<br>Ambari提供集中管理，用于在整个集群中启动，停止和重新配置Hadoop服务。<br>Ambari提供了一个仪表板，用于监控Hadoop集群的运行状况和状态。<br>Ambari利用Ambari指标系统进行指标收集。<br>Ambari利用Ambari Alert Framework进行系统警报，并在需要您注意时通知您（例如，节点出现故障，剩余磁盘空间不足等）。</p><h2 id="第2章-环境准备"><a href="#第2章-环境准备" class="headerlink" title="第2章 环境准备"></a>第2章 环境准备</h2><p><strong>以下操作三台机器都需要进行</strong><br><strong>2.1 虚拟机准备</strong><br>克隆三台虚拟机（hadoop102、hadoop103、hadoop104），配置好对应主机的网络IP、主机名称、关闭防火墙。</p><pre><code>[root@hadoop102 ~]# chkconfig iptables off[root@hadoop102 ~]# service iptables stop[root@hadoop102 ~]# chkconfig --list iptablesiptables        0:关闭  1:关闭  2:关闭  3:关闭  4:关闭  5:关闭  6:关闭</code></pre><p><strong>2.2 关闭SELINUX</strong></p><pre><code>[root@hadoop102 ~]# vim /etc/sysconfig/selinux将SELINUX=enforcing改为SELINUX=disabled</code></pre><p>执行该命令后重启机器生效</p><p><strong>2.3 安装JDK</strong><br>1）在hadoop102的/opt目录下创建module和software文件夹</p><pre><code>[root@hadoop102 opt]# mkdir module[root@hadoop102 opt]# mkdir software</code></pre><p>2）用SecureCRT将jdk-8u144-linux-x64.tar.gz导入到hadoop102的/opt/software目录下<br>3）在Linux系统下的opt目录中查看软件包是否导入成功</p><pre><code>[root@hadoop102 software]$ lsjdk-8u144-linux-x64.tar.gz</code></pre><p>4）解压JDK到/opt/module目录下</p><pre><code>[root@hadoop102 software]$ tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module/</code></pre><p>5）配置JDK环境变量<br>    （1）先获取JDK路径</p><pre><code>[root@hadoop102 jdk1.8.0_144]$ pwd    /opt/module/jdk1.8.0_144</code></pre><p>（2）打开/etc/profile文件</p><pre><code>[root@hadoop102 software]$ vi /etc/profile</code></pre><p>在profile文件末尾添加JDK路径</p><pre><code>#JAVA_HOMEexport JAVA_HOME=/opt/module/jdk1.8.0_144export PATH=$PATH:$JAVA_HOME/bin</code></pre><p>（3）保存后退出</p><pre><code>:wq</code></pre><p>（4）让修改后的文件生效</p><pre><code>[root@hadoop102 jdk1.8.0_144]$ source /etc/profile</code></pre><p>6）测试JDK是否安装成功</p><pre><code>[root@hadoop102 jdk1.8.0_144]# java -versionjava version &quot;1.8.0_144&quot;</code></pre><p>7）将hadoop102中的JDK和环境变量分发到hadoop103、hadoop104两台主机</p><pre><code>[root@hadoop102 opt]# xsync /opt/module/[root@hadoop102 opt]# xsync /etc/profilexysnc 是自定义的分发脚本</code></pre><p>分别在hadoop103、hadoop104上source一下</p><pre><code>[root@hadoop103 ~]$ source /etc/profile[root@hadoop104 ~]# source /etc/profile</code></pre><p><strong>2.4 SSH免密登录</strong><br>配置hadoop102对hadoop102、hadoop103、hadoop104三台主机的免密登陆。<br>（1）生成公钥和私钥：</p><pre><code>[root@hadoop102 .ssh]$ ssh-keygen -t rsa</code></pre><p>然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）<br>（2）将公钥拷贝到要免密登录的目标机器上</p><pre><code>[root@hadoop102 .ssh]$ ssh-copy-id hadoop102[root@hadoop102 .ssh]$ ssh-copy-id hadoop103[root@hadoop102 .ssh]$ ssh-copy-id hadoop104</code></pre><p><strong>2.5 修改yum源为阿里云镜像</strong></p><pre><code>[root@hadoop102 yum.repos.d]# mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bk[root@hadoop102 yum.repos.d]# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo[root@hadoop102 yum.repos.d]# yum makecache</code></pre><p><strong>2.6 安装ntp</strong></p><pre><code>[root@hadoop102 ~]# yum install -y ntp[root@hadoop102 ~]# chkconfig --list ntpd[root@hadoop102 ~]# chkconfig ntpd on[root@hadoop102 ~]# service ntpd start</code></pre><p><strong>2.7 关闭Linux的THP服务</strong><br>如果不关闭transparent_hugepage，HDFS会因为这个性能严重受影响。<br>关闭transparent_hugepage方法是：</p><pre><code>[root@hadoop102 ~]# vim /etc/grub.conf 添加 transparent_hugepage=never[root@hadoop102 ~]# vim /etc/rc.local添加：if test -f /sys/kernel/mm/transparent_hugepage/defrag; then  echo never &gt; /sys/kernel/mm/transparent_hugepage/defragfiif test -f /sys/kernel/mm/transparent_hugepage/enabled; then  echo never &gt; /sys/kernel/mm/transparent_hugepage/enabledfiexit 0</code></pre><p>重启之后，用下面的命令检查：</p><pre><code>[root@hadoop102 yum.repos.d]# cat /sys/kernel/mm/redhat_transparent_hugepage/enabledalways madvise [never]</code></pre><p>有 [never]则表示THP被禁用<br><strong>2.8 配置UMASK</strong><br>设定用户所创建目录的初始权限</p><pre><code>[root@hadoop102 ~]# umask 0022</code></pre><p><strong>2.9 禁止离线更新</strong></p><pre><code>vim /etc/yum/pluginconf.d/refresh-packagekit.conf修改：enabled=0</code></pre><h2 id="第3章-安装Ambari集群"><a href="#第3章-安装Ambari集群" class="headerlink" title="第3章 安装Ambari集群"></a>第3章 安装Ambari集群</h2><p><strong>以下操作主节点操作即可</strong><br><strong>3.1 制作本地源</strong><br>制作本地源是因为在线安装Ambari太慢。制作本地源只需在主节点上进行。<br>3.1.1 配置HTTP 服务<br>配置HTTP 服务到系统层使其随系统自动启动</p><pre><code>[root@hadoop102 ~]# chkconfig httpd on[root@hadoop102 ~]# service httpd start</code></pre><p>3.1.2 安装工具<br>安装本地源制作相关工具</p><pre><code>[root@hadoop102 ~]# yum install yum-utils createrepo yum-plugin-priorities -y[root@hadoop102 ~]# vim /etc/yum/pluginconf.d/priorities.conf添加gpgcheck=0</code></pre><p>3.1.3 将下载的3个tar包解压</p><pre><code>[root@hadoop102 software]# tar -zxvf /opt/software/ambari-2.5.0.3-centos6.tar.gz -C /var/www/html/[root@hadoop102 software]mkdir /var/www/html/hdp[root@hadoop102 software]# tar -zxvf /opt/software/HDP-2.6.0.3-centos6-rpm.tar.gz -C /var/www/html/hdp[root@hadoop102 software]# tar -zxvf /opt/software/HDP-UTILS-1.1.0.21-centos6.tar.gz -C /var/www/html/hdp</code></pre><p>3.1.4 创建本地源</p><pre><code>[root@hadoop102 software]# cd /var/www/html/[root@hadoop102 html]# createrepo  ./修改ambari.repo，配置为本地源[root@hadoop102 html]# vim /etc/yum.repos.d/ambari.repo#VERSION_NUMBER=2.6.1.5-3[ambari-2.6.1.5]name=ambari Version - ambari-2.6.1.5baseurl=http://hadoop102/ambari/centos6/gpgcheck=0gpgkey=http://hadoop102/ambari/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1修改hdp-util.repo，配置为本地源[root@hadoop102 hdp]# vim /var/www/html/hdp/hdp-util.repo[HDP-UTILS-1.1.0.21]name=Hortonworks Data Platform Version - HDP-UTILS-1.1.0.21baseurl=http://hadoop102/hdp/gpgcheck=0enabled=1priority=1修改hdp.repo，配置为本地源[root@hadoop102 centos6]# vim /var/www/html/hdp/HDP/centos6/hdp.repo#VERSION_NUMBER=2.6.0.3-8[HDP-2.6.0.3]name=HDP Version - HDP-2.6.0.3baseurl=http://hadoop102/hdp/HDP/centos6/gpgcheck=0gpgkey=http://hadoop102/hdp/HDP/centos6/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1[HDP-UTILS-1.1.0.21]name=HDP-UTILS Version - HDP-UTILS-1.1.0.21baseurl=http://hadoop102/hdp/gpgcheck=0gpgkey=http://hadoop102/hdp/RPM-GPG-KEY/RPM-GPG-KEY-Jenkinsenabled=1priority=1</code></pre><p>3.1.5 将Ambari存储库文件下载到安装主机上的目录中</p><pre><code>[root@hadoop102 yum.repos.d]# wget -nv http://public-repo-1.hortonworks.com/ambari/centos6/2.x/updates/2.6.1.5/ambari.repo -O /etc/yum.repos.d/ambari.repo[root@hadoop102 ~]# yum clean all[root@hadoop102 ~]# yum makecache查看是否有Ambari[root@hadoop102 ~]# yum repolist</code></pre><p>查看Ambari 与 HDP 资源的资源库。<br>也可以打开浏览器查看一下：<br><a href="http://hadoop102/ambari/centos6/" target="_blank" rel="noopener">http://hadoop102/ambari/centos6/</a><br><a href="http://hadoop102/hdp/HDP/centos6/" target="_blank" rel="noopener">http://hadoop102/hdp/HDP/centos6/</a><br><a href="http://hadoop102/hdp/" target="_blank" rel="noopener">http://hadoop102/hdp/</a><br><strong>3.2 安装MySQL</strong><br>Ambari使用的默认数据库是PostgreSQL，用于存储安装元数据，可以使用自己安装MySQL数据库作为Ambari元数据库。<br><strong>注意：一定要用root用户操作如下步骤；先卸载MySQL再安装</strong><br>1）安装包准备<br>    （1）查看MySQL是否安装</p><pre><code>[root@hadoop102 桌面]# rpm -qa|grep mysql    mysql-libs-5.1.73-7.el6.x86_64</code></pre><p>（2）如果安装了MySQL，就先卸载</p><pre><code>[root@hadoop102 桌面]# rpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64</code></pre><p>（3）解压mysql-libs.zip文件到当前目录</p><pre><code>[root@hadoop102 software]# unzip mysql-libs.zip[root@hadoop102 software]# lsmysql-libs.zipmysql-libs</code></pre><p>（4）进入到mysql-libs文件夹下</p><pre><code> [root@hadoop102 mysql-libs]# ll总用量 76048-rw-r--r--. 1 root root 18509960 3月  26 2015 MySQL-client-5.6.24-1.el6.x86_64.rpm-rw-r--r--. 1 root root  3575135 12月  1 2013 mysql-connector-java-5.1.27.tar.gz-rw-r--r--. 1 root root 55782196 3月  26 2015 MySQL-server-5.6.24-1.el6.x86_64.rpm</code></pre><p>2）安装MySQL服务器<br>（1）安装MySQL服务端</p><pre><code>[root@hadoop102 mysql-libs]# rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</code></pre><p>（2）查看产生的随机密码</p><pre><code>[root@hadoop102 mysql-libs]# cat /root/.mysql_secretOEXaQuS8IWkG19Xs</code></pre><p>（3）查看MySQL状态</p><pre><code>[root@hadoop102 mysql-libs]# service mysql status</code></pre><p>（4）启动MySQL</p><pre><code>[root@hadoop102 mysql-libs]# service mysql start</code></pre><p>3）安装MySQL客户端<br>（1）安装MySQL客户端</p><pre><code>[root@hadoop102 mysql-libs]# rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm</code></pre><p>（2）链接MySQL </p><pre><code>[root@hadoop102 mysql-libs]# mysql -uroot -pOEXaQuS8IWkG19Xs</code></pre><p>（3）修改密码</p><pre><code>mysql&gt;SET PASSWORD=PASSWORD(&#39;000000&#39;);</code></pre><p>（4）退出MySQL</p><pre><code>mysql&gt;exit</code></pre><p>4）MySQL中user表中主机配置<br>配置只要是root用户+密码，在任何主机上都能登录MySQL数据库。<br>（1）进入MySQL</p><pre><code>[root@hadoop102 mysql-libs]# mysql -uroot -p000000</code></pre><p>（2）显示数据库</p><pre><code>mysql&gt;show databases;</code></pre><p>（3）使用MySQL数据库</p><pre><code>mysql&gt;use mysql;</code></pre><p>（4）展示MySQL数据库中的所有表</p><pre><code>mysql&gt;show tables;</code></pre><p>（5）展示user表的结构</p><pre><code>mysql&gt;desc user;</code></pre><p>（6）查询user表</p><pre><code>mysql&gt;select User, Host, Password from user;</code></pre><p>（7）修改user表，把Host表内容修改为%</p><pre><code>mysql&gt;update user set host=&#39;%&#39; where host=&#39;localhost&#39;;</code></pre><p>（8）删除root用户的其他host</p><pre><code>mysql&gt;delete from user where Host=&#39;hadoop102&#39;;delete from user where Host=&#39;127.0.0.1&#39;;delete from user where Host=&#39;::1&#39;;</code></pre><p>（9）刷新</p><pre><code>mysql&gt;flush privileges;</code></pre><p>（10）退出</p><pre><code>mysql&gt;quit;</code></pre><p>3.3 安装Ambari<br>1）安装ambari-server</p><pre><code> [root@hadoop102 hdp]# yum install ambari-server</code></pre><p>2） 拷贝mysql驱动<br>将mysql-connector-java.jar复制到/usr/share/java目录下并改名为mysql-connector-java.jar</p><pre><code>[root@hadoop102 hdp]# mkdir /usr/share/java[root@hadoop102 hdp]# cp /opt/software/mysql-libs/mysql-connector-java-5.1.27/mysql-connector-java-5.1.27-bin.jar /usr/share/java/mysql-connector-java.jar</code></pre><p>将mysql-connector-java.jar复制到/var/lib/ambari-server/resources目录下并改名为mysql-jdbc-driver.jar</p><pre><code>[root@hadoop102 hdp]# cp /usr/share/java/mysql-connector-java.jar /var/lib/ambari-server/resources/mysql-jdbc-driver.jar</code></pre><p>修改ambari.properties文件</p><pre><code>[root@hadoop102 hdp]#vim     /etc/ambari-server/conf/ambari.properties</code></pre><p>添加</p><pre><code>server.jdbc.driver.path=/usr/share/java/mysql-connector-java.jar</code></pre><h2 id="3-4-在MySQL中创建数据库"><a href="#3-4-在MySQL中创建数据库" class="headerlink" title="3.4 在MySQL中创建数据库"></a>3.4 在MySQL中创建数据库</h2><p>1）创建ambari库</p><pre><code>[root@hadoop102 hdp]# mysql -u root -p000000 mysql &gt;create database ambari; </code></pre><p>2）使用Ambari自带脚本创建表</p><pre><code>mysql &gt;use ambari; mysql&gt;source /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql;</code></pre><p>3）赋予用户root权限：</p><pre><code>mysql&gt; grant all privileges on *.* to &#39;root&#39;@&#39;%&#39; identified by &#39;000000&#39;;</code></pre><p>4）刷新</p><pre><code>mysql&gt; flush privileges;</code></pre><h2 id="3-5-配置Ambari"><a href="#3-5-配置Ambari" class="headerlink" title="3.5 配置Ambari"></a>3.5 配置Ambari</h2><p>执行</p><pre><code>[root@hadoop102 hdp]# ambari-server setup</code></pre><p>下面是配置执行流程，按照提示操作</p><p>1） 提示是否自定义设置。输入：y </p><p><code>Customize user account for ambari-server daemon [y/n] (n)? y</code></p><p> 2）ambari-server 账号。 </p><pre><code>Enter user account for ambari-server daemon (root):</code></pre><p> 如果直接回车就是默认选择root用户 </p><p>3）检查防火墙是否关闭 </p><p><code>Adjusting ambari-server permissions and ownership... Checking firewall... WARNING: iptables is running. Confirm the necessary Ambari ports are accessible. Refer to the Ambari documentation for more details on ports. OK to continue [y/n] (y)?</code> </p><p>直接回车 </p><p>4）设置JDK。输入：3 </p><pre><code>Checking JDK... Do you want to change Oracle JDK [y/n] (n)? y [1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8 [2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7 [3] Custom JDK ============================================================================== Enter choice (1): 3 </code></pre><p>如果上面选择3自定义JDK,则需要设置JAVA_HOME。输入：/opt/module/jdk1.8.0_144 <code>WARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts. WARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts. Path to JAVA_HOME: /opt/module/jdk1.8.0_144 Validating JDK on Ambari Server...done. Completing setup...</code> </p><p>5）数据库配置。选择：y </p><pre><code>Configuring database... Enter advanced database configuration [y/n] (n)? y </code></pre><p>6）选择数据库类型。输入：3 </p><pre><code>Configuring database... ============================================================================== Choose one of the following options: [1] - PostgreSQL (Embedded) [2] - Oracle [3] - MySQL [4] - PostgreSQL [5] - Microsoft SQL Server (Tech Preview) [6] - SQL Anywhere ==============================================================================     Enter choice (3): 3 </code></pre><p>7）设置数据库的具体配置信息，根据实际情况输入，如果和括号内相同，则可以直接回车。如果想重命名，就输入。 </p><pre><code>Hostname (localhost):hadoop102 Port (3306): Database name (ambari): Username (ambari):root Enter Database Password (bigdata): Re-Enter password: </code></pre><p>8）将Ambari数据库脚本导入到数据库 </p><p><code>WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql Proceed with configuring remote database connection properties [y/n] (y)?</code> </p><p>如果使用自己定义的数据库，必须在启动Ambari服务之前导入Ambari的sql脚本。 </p><p><strong>3.6 启动Ambari</strong> </p><p>启动命令为： <code>ambari-server start</code> 停止命令为： <code>ambari-server stop</code> </p><h1 id="第4章-HDP集群部署"><a href="#第4章-HDP集群部署" class="headerlink" title="第4章 HDP集群部署"></a>第4章 HDP集群部署</h1><p>4.1 集群搭建 </p><p>4.1.1进入登录页面 浏览器输入<a href="http://hadoop102:8080/" target="_blank" rel="noopener">http://hadoop102:8080/</a> </p><p>默认管理员账目密码：admin</p><p><img src="https://img-blog.csdnimg.cn/20190509172907284.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" srcset="undefined" alt="在这里插入图片描述"><br>4.1.2 点击Launch Install Wizard<br> <img src="https://img-blog.csdnimg.cn/20190509172915630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" srcset="undefined" alt="在这里插入图片描述"><br>4.1.3 设置集群名称<br> <img src="https://img-blog.csdnimg.cn/20190509172921642.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" srcset="undefined" alt="在这里插入图片描述"><br>4.1.4选择版本和存储库<br> <img src="https://img-blog.csdnimg.cn/20190509172930677.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" srcset="undefined" alt="在这里插入图片描述"><br>4.1.5 填写本地库地址<br>在redhat6后面分别填写<br><a href="http://hadoop102/hdp/HDP/centos6/" target="_blank" rel="noopener">http://hadoop102/hdp/HDP/centos6/</a><br><a href="http://hadoop102/hdp/" target="_blank" rel="noopener">http://hadoop102/hdp/</a></p><p> <img src="https://img-blog.csdnimg.cn/20190509172937156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" srcset="undefined" alt="在这里插入图片描述"><br>4.1.6 填写主机地址以及主节点的id.rsa文件<br> <img src="https://img-blog.csdnimg.cn/20190509172941791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" srcset="undefined" alt="在这里插入图片描述"><br>4.1.7 等待安装<br> <img src="https://img-blog.csdnimg.cn/20190509172948628.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" srcset="undefined" alt="在这里插入图片描述"><br>4.1.8 选择服务<br> <img src="https://img-blog.csdnimg.cn/20190509172954631.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" srcset="undefined" alt="在这里插入图片描述"><br>4.1.9 选择每台机器的角色<br> <img src="https://img-blog.csdnimg.cn/20190509172959265.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" srcset="undefined" alt="在这里插入图片描述"><br>4.1.10 设置从节点<br> <img src="https://img-blog.csdnimg.cn/20190509173005164.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" srcset="undefined" alt="在这里插入图片描述"><br>4.1.11 输入两次admin</p><p> <img src="https://img-blog.csdnimg.cn/20190509173009273.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" srcset="undefined" alt="在这里插入图片描述"><br> <img src="https://img-blog.csdnimg.cn/20190509173024816.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" srcset="undefined" alt="在这里插入图片描述"><br>4.1.12 继续<br> <img src="https://img-blog.csdnimg.cn/20190509173030830.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" srcset="undefined" alt="在这里插入图片描述"><br>4.1.13 等待服务安装和启动<br> <img src="https://img-blog.csdnimg.cn/20190509173035505.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d5ZWUwMDA=,size_16,color_FFFFFF,t_70" srcset="undefined" alt="在这里插入图片描述"><br>4.2 安装Hive<br>待续。。。。</p>]]></content>
    
    
    <categories>
      
      <category>Ambari</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Ambari</tag>
      
      <tag>BigData</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>clickhouse的安装和使用（单机+集群）</title>
    <link href="undefined2019/05/08/clickhouse%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8%EF%BC%88%E5%8D%95%E6%9C%BA+%E9%9B%86%E7%BE%A4%EF%BC%89/"/>
    <url>2019/05/08/clickhouse%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8%EF%BC%88%E5%8D%95%E6%9C%BA+%E9%9B%86%E7%BE%A4%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="什么是clickhous"><a href="#什么是clickhous" class="headerlink" title="什么是clickhous"></a>什么是clickhous</h2><pre><code> ClickHouse 是俄罗斯的Yandex于2016年开源的列式存储数据库（DBMS），主要用于在线分析处理查询（OLAP），能够使用SQL查询实时生成分析数据报告。</code></pre><h2 id="安装前的准备"><a href="#安装前的准备" class="headerlink" title="安装前的准备"></a>安装前的准备</h2><p>以CentOS6.8为例</p><p> <strong>1. <strong>CentOS取消打开文件数限制</strong></strong><br>    在/etc/security/limits.conf、/etc/security/limits.d/90-nproc.conf这2个文件的末尾加入一下内容：</p><ul><li><p>soft nofile 65536 </p></li><li><p>hard nofile 65536 </p></li><li><p>soft nproc 131072 </p></li><li><p>hard nproc 131072<br>重启生效 用ulimit –n 或者ulimit –a查看设置结果<br>用ulimit –n 或者ulimit –a查看设置结果</p><pre><code>[root@hadoop102 ~]# ulimit -n65536</code></pre></li><li><p><em>2. CentOS取消取消SELINU*</em></p><pre><code>修改/etc/selinux/config中的SELINUX=disabled后重启</code></pre><p>   vim /etc/selinux/config<br>   SELINUX=disabled</p></li><li><p><em>3. CentOS关闭防火墙*</em></p><pre><code>service iptables stop service ip6tables stop</code></pre></li></ul><p><strong>4. 安装依赖</strong></p><pre><code>   yum install -y libtool   yum install -y *unixODBC*</code></pre><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><strong>1.网址</strong></p><p><a href="https://clickhouse.yandex/" target="_blank" rel="noopener">官网</a><br><a href="https://packagecloud.io/altinity/clickhouse" target="_blank" rel="noopener">安装包下载地址</a></p><p><strong>2.单机模式</strong></p><p><strong>上传5个文件到Linux中</strong></p><pre><code>[root@hadoop102 software]# lsclickhouse-client-1.1.54236-4.el6.x86_64.rpm      clickhouse-server-1.1.54236-4.el6.x86_64.rpmclickhouse-compressor-1.1.54236-4.el6.x86_64.rpm  clickhouse-server-common-1.1.54236-4.el6.x86_64.rpmclickhouse-debuginfo-1.1.54236-4.el6.x86_64.rpm</code></pre><p><strong>分别安装这5个rpm文件</strong></p><pre><code>[root@hadoop102 software]#  rpm -ivh *.rpm </code></pre><p><strong>启动ClickServer</strong><br>前台启动：</p><pre><code>clickhouse-server –config-file=/etc/clickhouse-server/config.xml</code></pre><p>后台启动：</p><pre><code>nohup clickhouse-server –config-file=/etc/clickhouse-server/config.xml  &gt;null 2&gt;&amp;1 [1] 2696</code></pre><p><strong>使用client连接server</strong></p><pre><code>clickhouse-client </code></pre><p><strong>3.分布式安装</strong></p><p><strong>准备三台机器，改好主机名之类的，然后执行以上所有步骤</strong><br>我这里是hadoop102,hadoop103,hadoop104</p><p><strong>三台机器修改配置文件config.xml</strong></p><pre><code>vim /etc/clickhouse-server/config.xml把60行左右的三行改为这样    &lt;listen_host&gt;::&lt;/listen_host&gt;    &lt;!-- &lt;listen_host&gt;::1&lt;/listen_host&gt; --&gt;    &lt;!-- &lt;listen_host&gt;127.0.0.1&lt;/listen_host&gt; --&gt;</code></pre><p><strong>在三台机器的etc目录下新建metrika.xml文件</strong></p><pre><code>vim /etc/metrika.xml添加如下内容：&lt;yandex&gt;&lt;clickhouse_remote_servers&gt;    &lt;perftest_3shards_1replicas&gt;        &lt;shard&gt;             &lt;internal_replication&gt;true&lt;/internal_replication&gt;            &lt;replica&gt;                &lt;host&gt;hadoop102&lt;/host&gt;                &lt;port&gt;9000&lt;/port&gt;            &lt;/replica&gt;        &lt;/shard&gt;        &lt;shard&gt;            &lt;replica&gt;                &lt;internal_replication&gt;true&lt;/internal_replication&gt;                &lt;host&gt;hadoop103&lt;/host&gt;                &lt;port&gt;9000&lt;/port&gt;            &lt;/replica&gt;        &lt;/shard&gt;        &lt;shard&gt;            &lt;internal_replication&gt;true&lt;/internal_replication&gt;            &lt;replica&gt;                &lt;host&gt;hadoop104&lt;/host&gt;                &lt;port&gt;9000&lt;/port&gt;            &lt;/replica&gt;        &lt;/shard&gt;    &lt;/perftest_3shards_1replicas&gt;&lt;/clickhouse_remote_servers&gt;&lt;zookeeper-servers&gt;  &lt;node index=&quot;1&quot;&gt;    &lt;host&gt;hadoop102&lt;/host&gt;    &lt;port&gt;2181&lt;/port&gt;  &lt;/node&gt;  &lt;node index=&quot;2&quot;&gt;    &lt;host&gt;hadoop103&lt;/host&gt;    &lt;port&gt;2181&lt;/port&gt;  &lt;/node&gt;  &lt;node index=&quot;3&quot;&gt;    &lt;host&gt;hadoop104&lt;/host&gt;    &lt;port&gt;2181&lt;/port&gt;  &lt;/node&gt;&lt;/zookeeper-servers&gt;&lt;macros&gt;    &lt;replica&gt;hadoop102&lt;/replica&gt;&lt;/macros&gt;&lt;networks&gt;   &lt;ip&gt;::/0&lt;/ip&gt;&lt;/networks&gt;&lt;clickhouse_compression&gt;&lt;case&gt;  &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt;  &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt;                                                                                                                                         &lt;method&gt;lz4&lt;/method&gt;&lt;/case&gt;&lt;/clickhouse_compression&gt;&lt;/yandex&gt;</code></pre><p><strong>注意：</strong>    </p><pre><code>&lt;macros&gt;    &lt;replica&gt;hadoop102&lt;/replica&gt;&lt;/macros&gt;</code></pre><p>不同机器这里不能相同</p><p><strong>三台机器启动ClickServer</strong></p><pre><code>service clickhouse-server start</code></pre><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p> <strong>整型</strong><br>固定长度的整型，包括有符号整型或无符号整型。<br>整型范：<br>Int8 - [-128 : 127]<br>Int16 - [-32768 : 32767]<br>Int32 - [-2147483648 : 2147483647]<br>Int64 - [-9223372036854775808 : 9223372036854775807]<br>无符号整型范：<br>UInt8 - [0 : 255]<br>UInt16 - [0 : 65535]<br>UInt32 - [0 : 4294967295]<br>UInt64 - [0 : 18446744073709551615]</p><p> <strong>浮点型</strong><br>Float32 - float<br>Float64 – double<br>建议尽可能以整数形式存储数据。例如，将固定精度的数字转换为整数值，如时间用毫秒为单位表示，因为浮点型进行计算时可能引起四舍五入的误差。</p><pre><code>select 1-0.9┌───────minus(1, 0.9)─┐│ 0.09999999999999998 │└─────────────────────┘</code></pre><p>与标准SQL相比，ClickHouse 支持以下类别的浮点数：</p><p>Inf-正无穷</p><pre><code>select 1/0┌─divide(1, 0)─┐│          inf │└──────────────┘</code></pre><p>-Inf-负无穷：</p><pre><code>select -1/0┌─divide(1, 0)─┐│          -inf │└──────────────┘</code></pre><p>NaN-非数字：</p><pre><code>:) select 0/0┌─divide(0, 0)─┐│          nan │└──────────────┘</code></pre><p><strong>布尔型</strong><br>没有单独的类型来存储布尔值。可以使用 UInt8 类型，取值限制为 0 或 1。</p><p><strong>字符串</strong><br>String<br>    字符串可以任意长度的。它可以包含任意的字节集，包含空字节。<br>FixedString(N)<br>    固定长度 N 的字符串，N 必须是严格的正自然数。当服务端读取长度小于 N 的字符串时候，通过在字符串末尾添加空字节来达到 N 字节长度。 当服务端读取长度大于 N 的字符串时候，将返回错误消息。<br>    与String相比，极少会使用FixedString，因为使用起来不是很方便。</p><p><strong>枚举类型</strong><br>包括 Enum8 和 Enum16 类型<br>Enum 保存 ‘string’= integer 的对应关系。<br>Enum8 用 ‘String’= Int8 对描述。<br>Enum16 用 ‘String’= Int16 对描述。<br>用法演示：<br>创建一个带有一个枚举 Enum8(‘hello’ = 1, ‘world’ = 2) 类型的列：</p><pre><code>CREATE TABLE t_enum(    x Enum8(&#39;hello&#39; = 1, &#39;world&#39; = 2))ENGINE = TinyLog</code></pre><p>这个 x 列只能存储类型定义中列出的值：’hello’或’world’。如果尝试保存任何其他值，ClickHouse 抛出异常。</p><pre><code>    INSERT INTO t_enum VALUES (&#39;hello&#39;), (&#39;world&#39;), (&#39;hello&#39;)    INSERT INTO t_enum VALUES    Ok.3 rows in set. Elapsed: 0.002 sec.insert into t_enum values(&#39;a&#39;)INSERT INTO t_enum VALUESException on client:Code: 49. DB::Exception: Unknown element &#39;a&#39; for type Enum8(&#39;hello&#39; = 1, &#39;world&#39; = 2)</code></pre><p>从表中查询数据时，ClickHouse 从 Enum 中输出字符串值。</p><pre><code>SELECT * FROM t_enum┌─x─────┐│ hello ││ world ││ hello │└───────┘</code></pre><p>如果需要看到对应行的数值，则必须将 Enum 值转换为整数类型。</p><pre><code>SELECT CAST(x, &#39;Int8&#39;) FROM t_enum┌─CAST(x, &#39;Int8&#39;)─┐│               1 ││               2 ││               1 │└─────────────────┘</code></pre><p><strong>数组</strong><br>Array(T)：由 T 类型元素组成的数组。<br>T 可以是任意类型，包含数组类型。 但不推荐使用多维数组，ClickHouse 对多维数组的支持有限。例如，不能在 MergeTree 表中存储多维数组。<br>可以使用array函数来创建数组：</p><pre><code>array(T)</code></pre><p>也可以使用方括号：</p><pre><code>[]</code></pre><p>创建数组案例：</p><pre><code>SELECT array(1, 2) AS x, toTypeName(x)SELECT    [1, 2] AS x,    toTypeName(x)┌─x─────┬─toTypeName(array(1, 2))─┐│ [1,2] │ Array(UInt8)            │└───────┴─────────────────────────┘1 rows in set. Elapsed: 0.002 sec.:) SELECT [1, 2] AS x, toTypeName(x)SELECT    [1, 2] AS x,    toTypeName(x)┌─x─────┬─toTypeName([1, 2])─┐│ [1,2] │ Array(UInt8)       │└───────┴────────────────────┘1 rows in set. Elapsed: 0.002 sec.</code></pre><p><strong>元组</strong><br>Tuple(T1, T2, …)：元组，其中每个元素都有单独的类型。<br>创建元组的示例：</p><pre><code>:) SELECT tuple(1,&#39;a&#39;) AS x, toTypeName(x)SELECT    (1, &#39;a&#39;) AS x,    toTypeName(x)┌─x───────┬─toTypeName(tuple(1, &#39;a&#39;))─┐│ (1,&#39;a&#39;) │ Tuple(UInt8, String)      │└─────────┴───────────────────────────┘1 rows in set. Elapsed: 0.021 sec.</code></pre><p><strong>Date</strong><br>日期类型，用两个字节存储，表示从 1970-01-01 (无符号) 到当前的日期值。</p><p>还有很多数据结构，可以参考官方文档 ：<a href="https://clickhouse.yandex/docs/zh/data_types/" target="_blank" rel="noopener">官方文档</a></p><h2 id="表引擎"><a href="#表引擎" class="headerlink" title="表引擎"></a>表引擎</h2><p>表引擎（即表的类型）决定了：<br>1）数据的存储方式和位置，写到哪里以及从哪里读取数据<br>2）支持哪些查询以及如何支持。<br>3）并发数据访问。<br>4）索引的使用（如果存在）。<br>5）是否可以执行多线程请求。<br>6）数据复制参数。<br>ClickHouse的表引擎有很多，下面介绍其中几种，对其他引擎有兴趣的可以去查阅官方文档：<a href="https://clickhouse.yandex/docs/zh/operations/table_engines/" target="_blank" rel="noopener">官方文档</a></p><p><strong>TinyLog</strong><br>最简单的表引擎，用于将数据存储在磁盘上。每列都存储在单独的压缩文件中，写入时，数据将附加到文件末尾。<br>该引擎没有并发控制 </p><ul><li>如果同时从表中读取和写入数据，则读取操作将抛出异常；</li><li>如果同时写入多个查询中的表，则数据将被破坏。</li></ul><p>这种表引擎的典型用法是 write-once：首先只写入一次数据，然后根据需要多次读取。此引擎适用于相对较小的表（建议最多1,000,000行）。如果有许多小表，则使用此表引擎是适合的，因为它比需要打开的文件更少。当拥有大量小表时，可能会导致性能低下。      不支持索引。<br>案例：创建一个TinyLog引擎的表并插入一条数据</p><pre><code>:)create table t (a UInt16, b String) ENGINE = TinyLog;:)insert into t (a, b) values (1, &#39;abc&#39;);</code></pre><p>此时我们到保存数据的目录<code>/var/lib/clickhouse/data/default/t</code>中可以看到如下目录结构：</p><pre><code>[root@hadoop102 t]# lsa.bin  b.bin  sizes.json</code></pre><p>a.bin 和 b.bin 是压缩过的对应的列的数据， sizes.json 中记录了每个 *.bin 文件的大小：</p><pre><code>[root@hadoop102 t]# cat sizes.json {&quot;yandex&quot;:{&quot;a%2Ebin&quot;:{&quot;size&quot;:&quot;28&quot;},&quot;b%2Ebin&quot;:{&quot;size&quot;:&quot;30&quot;}}}</code></pre><p><strong>Memory</strong><br>       内存引擎，数据以未压缩的原始形式直接保存在内存当中，服务器重启数据就会消失。读写操作不会相互阻塞，不支持索引。简单查询下有非常非常高的性能表现（超过10G/s）。<br>        一般用到它的地方不多，除了用来测试，就是在需要非常高的性能，同时数据量又不太大（上限大概 1 亿行）的场景。</p><p><strong>Merge</strong><br>Merge 引擎 (不要跟 MergeTree 引擎混淆) 本身不存储数据，但可用于同时从任意多个其他的表中读取数据。 读是自动并行的，不支持写入。读取时，那些被真正读取到数据的表的索引（如果有的话）会被使用。<br>Merge 引擎的参数：一个数据库名和一个用于匹配表名的正则表达式。<br>案例：先建t1，t2，t3三个表，然后用 Merge 引擎的 t 表再把它们链接起来。</p><pre><code>:)create table t1 (id UInt16, name String) ENGINE=TinyLog;:)create table t2 (id UInt16, name String) ENGINE=TinyLog;:)create table t3 (id UInt16, name String) ENGINE=TinyLog;:)insert into t1(id, name) values (1, &#39;first&#39;);:)insert into t2(id, name) values (2, &#39;second&#39;);:)insert into t3(id, name) values (3, &#39;i am in t3&#39;);:)create table t (id UInt16, name String) ENGINE=Merge(currentDatabase(), &#39;^t&#39;);:) select * from t;┌─id─┬─name─┐│  2 │ second │└────┴──────┘┌─id─┬─name──┐│  1 │ first │└────┴───────┘┌─id─┬─name───────┐│ 3     │ i am in t3 │└────┴────────────┘</code></pre><p><strong>MergeTree</strong><br>Clickhouse 中最强大的表引擎当属 MergeTree （合并树）引擎及该系列（*MergeTree）中的其他引擎。<br>MergeTree 引擎系列的基本理念如下。当你有巨量数据要插入到表中，你要高效地一批批写入数据片段，并希望这些数据片段在后台按照一定规则合并。相比在插入时不断修改（重写）数据进存储，这种策略会高效很多。<br>格式：</p><pre><code>`ENGINE [=] MergeTree(date-column [, sampling_expression], (primary, key),` index_granularity)</code></pre><p>参数解读：</p><pre><code>date-column — 类型为 Date 的列名。ClickHouse 会自动依据这个列按月创建分区。分区名格式为 &quot;YYYYMM&quot; 。sampling_expression — 采样表达式。(primary, key) — 主键。类型为Tuple()index_granularity — 索引粒度。即索引中相邻”标记”间的数据行数。设为 8192 可以适用大部分场景。</code></pre><p>案例：</p><pre><code>create table mt_table (date  Date, id UInt8, name String) ENGINE=MergeTree(date, (id, name), 8192);insert into mt_table values (&#39;2019-05-01&#39;, 1, &#39;zhangsan&#39;);insert into mt_table values (&#39;2019-06-01&#39;, 2, &#39;lisi&#39;);insert into mt_table values (&#39;2019-05-03&#39;, 3, &#39;wangwu&#39;);在/var/lib/clickhouse/data/default/mt_tree下可以看到：[root@hadoop102 mt_table]# ls20190501_20190501_2_2_0  20190503_20190503_6_6_0  20190601_20190601_4_4_0  detached</code></pre><p>随便进入一个目录：</p><pre><code>[root@hadoop102 20190601_20190601_4_4_0]# lschecksums.txt  columns.txt  date.bin  date.mrk  id.bin  id.mrk  name.bin  name.mrk  primary.idx</code></pre><ul><li>*.bin是按列保存数据的文件</li><li>*.mrk保存块偏移量</li><li>primary.idx保存主键索引</li></ul><p><strong>ReplacingMergeTree</strong><br>这个引擎是在 MergeTree 的基础上，添加了“处理重复数据”的功能，该引擎和MergeTree的不同之处在于它会删除具有相同主键的重复项。数据的去重只会在合并的过程中出现。合并会在未知的时间在后台进行，所以你无法预先作出计划。有一些数据可能仍未被处理。因此，ReplacingMergeTree 适用于在后台清除重复的数据以节省空间，但是它不保证没有重复的数据出现。<br>格式：</p><pre><code>ENGINE [=] ReplacingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, [ver])</code></pre><p>可以看出他比MergeTree只多了一个ver，这个ver指代版本列。<br>案例：</p><pre><code>create table rmt_table (date  Date, id UInt8, name String,point UInt8) ENGINE= ReplacingMergeTree(date, (id, name), 8192,point);插入一些数据：insert into rmt_table values (&#39;2019-07-10&#39;, 1, &#39;a&#39;, 20);insert into rmt_table values (&#39;2019-07-10&#39;, 1, &#39;a&#39;, 30);insert into rmt_table values (&#39;2019-07-11&#39;, 1, &#39;a&#39;, 20);insert into rmt_table values (&#39;2019-07-11&#39;, 1, &#39;a&#39;, 30);insert into rmt_table values (&#39;2019-07-11&#39;, 1, &#39;a&#39;, 10);等待一段时间或optimize table rmt_table手动触发merge，后查询:) select * from rmt_table;┌───────date─┬─id─┬─name─┬─point─┐│ 2019-07-11 │  1 │ a    │    30 │└────────────┴────┴──────┴───────┘</code></pre><p><strong>SummingMergeTree</strong><br>该引擎继承自 MergeTree。区别在于，当合并 SummingMergeTree 表的数据片段时，ClickHouse 会把所有具有相同主键的行合并为一行，该行包含了被合并的行中具有数值数据类型的列的汇总值。如果主键的组合方式使得单个键值对应于大量的行，则可以显著的减少存储空间并加快数据查询的速度，对于不可加的列，会取一个最先出现的值。<br>语法：</p><pre><code>ENGINE [=] SummingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, [columns])</code></pre><p>参数：</p><pre><code>columns — 包含将要被汇总的列的列名的元组</code></pre><p>案例：</p><pre><code>create table smt_table (date Date, name String, a UInt16, b UInt16) ENGINE=SummingMergeTree(date, (date, name), 8192, (a))</code></pre><p>插入数据：</p><pre><code>insert into smt_table (date, name, a, b) values (&#39;2019-07-10&#39;, &#39;a&#39;, 1, 2);insert into smt_table (date, name, a, b) values (&#39;2019-07-10&#39;, &#39;b&#39;, 2, 1);insert into smt_table (date, name, a, b) values (&#39;2019-07-11&#39;, &#39;b&#39;, 3, 8);insert into smt_table (date, name, a, b) values (&#39;2019-07-11&#39;, &#39;b&#39;, 3, 8);insert into smt_table (date, name, a, b) values (&#39;2019-07-11&#39;, &#39;a&#39;, 3, 1);insert into smt_table (date, name, a, b) values (&#39;2019-07-12&#39;, &#39;c&#39;, 1, 3);</code></pre><p>等待一段时间或optimize table smt_table手动触发merge，后查询</p><pre><code>:) select * from smt_table ┌───────date─┬─name─┬─a─┬─b─┐│ 2019-07-10 │ a    │ 1 │ 2 ││ 2019-07-10 │ b    │ 2 │ 1 ││ 2019-07-11 │ a    │ 3 │ 1 ││ 2019-07-11 │ b    │ 6 │ 8 ││ 2019-07-12 │ c    │ 1 │ 3 │└────────────┴──────┴───┴───┘</code></pre><p>发现2019-07-11，b的a列合并相加了，b列取了8（因为b列为8的数据最先插入）。</p><p> <strong>Distributed</strong><br>分布式引擎，本身不存储数据, 但可以在多个服务器上进行分布式查询。 读是自动并行的。读取时，远程服务器表的索引（如果有的话）会被使用。 </p><pre><code>Distributed(cluster_name, database, table [, sharding_key])</code></pre><p>参数解析：</p><pre><code>cluster_name  - 服务器配置文件中的集群名,在/etc/metrika.xml中配置的database – 数据库名table – 表名sharding_key – 数据分片键</code></pre><p>案例演示：<br>1）在hadoop102，hadoop103，hadoop104上分别创建一个表t</p><pre><code>:)create table t(id UInt16, name String) ENGINE=TinyLog;</code></pre><p>2）在三台机器的t表中插入一些数据</p><pre><code>:)insert into t(id, name) values (1, &#39;zhangsan&#39;);:)insert into t(id, name) values (2, &#39;lisi&#39;);</code></pre><p>3）在hadoop102上创建分布式表</p><pre><code>:)create table dis_table(id UInt16, name String) ENGINE=Distributed(perftest_3shards_1replicas, default, t, id);</code></pre><p>4）往dis_table中插入数据</p><pre><code>:) insert into dis_table select * from t</code></pre><p>5）查看数据量</p><pre><code>:) select count() from dis_table FROM dis_table ┌─count()─┐│       8 │└─────────┘:) select count() from tSELECT count()FROM t ┌─count()─┐│       3 │└─────────┘</code></pre><p>可以看到每个节点大约有1/3的数据</p><h2 id="SQL语法"><a href="#SQL语法" class="headerlink" title="SQL语法"></a>SQL语法</h2><p><strong>CREATE</strong></p><p>CREATE DATABASE<br>用于创建指定名称的数据库，语法如下：</p><pre><code>CREATE DATABASE [IF NOT EXISTS] db_name</code></pre><p>如果查询中存在IF NOT EXISTS，则当数据库已经存在时，该查询不会返回任何错误。</p><pre><code>:) create database test;Ok.0 rows in set. Elapsed: 0.018 sec.</code></pre><p> CREATE TABLE<br>对于创建表，语法如下：</p><pre><code>CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster](    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],    ...) ENGINE = engineDEFAULT expr – 默认值，用法与SQL类似。MATERIALIZED expr – 物化表达式，被该表达式指定的列不能被INSERT，因为它总是被计算出来的。 对于INSERT而言，不需要考虑这些列。 另外，在SELECT查询中如果包含星号，此列不会被查询。ALIAS expr – 别名。</code></pre><p>有三种方式创建表：<br>1）直接创建</p><pre><code>:) create table t1(id UInt16,name String) engine=TinyLog</code></pre><p>2）创建一个与其他表具有相同结构的表</p><pre><code>CREATE TABLE [IF NOT EXISTS] [db.]table_name AS [db2.]name2 [ENGINE = engine]</code></pre><p>可以对其指定不同的表引擎声明。如果没有表引擎声明，则创建的表将与db2.name2使用相同的表引擎。</p><pre><code>:) create table t2 as t1 engine=Memory:) desc t2DESCRIBE TABLE t2┌─name─┬─type───┬─default_type─┬─default_expression─┐│ id   │ UInt16 │              │                    ││ name   │ String │              │                    │└──────┴────────┴──────────────┴────────────────────┘</code></pre><p>3）使用指定的引擎创建一个与SELECT子句的结果具有相同结构的表，并使用SELECT子句的结果填充它。<br>语法：</p><pre><code>CREATE TABLE [IF NOT EXISTS] [db.]table_name ENGINE = engine AS SELECT ...</code></pre><p>实例：<br>先在t2中插入几条数据</p><pre><code>:) insert into t1 values(1,&#39;zhangsan&#39;),(2,&#39;lisi&#39;),(3,&#39;wangwu&#39;):) create table t3 engine=TinyLog as select * from t1:) select * from t3┌─id─┬─name─────┐│  1 │ zhangsan ││  2 │ lisi     ││  3 │ wangwu   │└────┴──────────┘</code></pre><p> <strong>INSERT INTO</strong><br>主要用于向表中添加数据，基本格式如下：</p><pre><code>INSERT INTO [db.]table [(c1, c2, c3)] VALUES (v11, v12, v13), (v21, v22, v23), ...</code></pre><p>实例：</p><pre><code>:) insert into t1 values(1,&#39;zhangsan&#39;),(2,&#39;lisi&#39;),(3,&#39;wangwu&#39;)</code></pre><p>还可以使用select来写入数据：</p><pre><code>INSERT INTO [db.]table [(c1, c2, c3)] SELECT ...    实例：:) insert into t2 select * from t3:) select * from t2┌─id─┬─name─────┐│  1 │ zhangsan ││  2 │ lisi     ││  3 │ wangwu   │└────┴──────────┘</code></pre><p>ClickHouse不支持的修改数据的查询：UPDATE, DELETE, REPLACE, MERGE, UPSERT, INSERT UPDATE。</p><p><strong>ALTER</strong><br>ALTER只支持MergeTree系列，Merge和Distributed引擎的表，基本语法：</p><pre><code>ALTER TABLE [db].name [ON CLUSTER cluster] ADD|DROP|MODIFY COLUMN ...</code></pre><p>参数解析：</p><pre><code>ADD COLUMN – 向表中添加新列DROP COLUMN – 在表中删除列MODIFY COLUMN – 更改列的类型</code></pre><p>案例演示：<br>1）创建一个MergerTree引擎的表</p><pre><code>create table mt_table (date  Date, id UInt8, name String) ENGINE=MergeTree(date, (id, name), 8192);</code></pre><p>2）向表中插入一些值</p><pre><code>insert into mt_table values (&#39;2019-05-01&#39;, 1, &#39;zhangsan&#39;);insert into mt_table values (&#39;2019-06-01&#39;, 2, &#39;lisi&#39;);insert into mt_table values (&#39;2019-05-03&#39;, 3, &#39;wangwu&#39;);</code></pre><p>3）在末尾添加一个新列age</p><pre><code>:)alter table mt_table add column age UInt8:)desc mt_table┌─name─┬─type───┬─default_type─┬─default_expression─┐│ date │ Date   │              │                    ││ id   │ UInt8  │              │                    ││ name │ String │              │                    ││ age  │ UInt8  │              │                    │└──────┴────────┴──────────────┴────────────────────┘:) select * from mt_table┌───────date─┬─id─┬─name─┬─age─┐│ 2019-06-01 │  2 │ lisi │   0 │└────────────┴────┴──────┴─────┘┌───────date─┬─id─┬─name─────┬─age─┐│ 2019-05-01 │  1 │ zhangsan │   0 ││ 2019-05-03 │  3 │ wangwu   │   0 │└────────────┴────┴──────────┴─────┘</code></pre><p>4）更改age列的类型</p><pre><code>:)alter table mt_table modify column age UInt16:)desc mt_table┌─name─┬─type───┬─default_type─┬─default_expression─┐│ date │ Date   │              │                    ││ id   │ UInt8  │              │                    ││ name │ String │              │                    ││ age  │ UInt16 │              │                    │└──────┴────────┴──────────────┴────────────────────┘</code></pre><p>5）删除刚才创建的age列</p><pre><code>:)alter table mt_table drop column age:)desc mt_table┌─name─┬─type───┬─default_type─┬─default_expression─┐│ date │ Date   │              │                    ││ id   │ UInt8  │              │                    ││ name │ String │              │                    │└──────┴────────┴──────────────┴────────────────────┘</code></pre><p><strong>DESCRIBE TABLE</strong><br>查看表结构</p><pre><code>:)desc mt_table┌─name─┬─type───┬─default_type─┬─default_expression─┐│ date │ Date   │              │                    ││ id   │ UInt8  │              │                    ││ name │ String │              │                    │└──────┴────────┴──────────────┴────────────────────┘</code></pre><p> <strong>CHECK TABLE</strong><br>检查表中的数据是否损坏，他会返回两种结果：<br>0 – 数据已损坏<br>1 – 数据完整<br>该命令只支持Log，TinyLog和StripeLog引擎。</p>]]></content>
    
    
    <categories>
      
      <category>数据库</category>
      
    </categories>
    
    
    <tags>
      
      <tag>-clickhouse</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>